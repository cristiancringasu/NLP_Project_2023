{"cells":[{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["import json\n","import pprint\n","\n","import datetime\n","import requests\n","import gzip\n","import subprocess\n","import re\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import yfinance as yf\n","from finta import TA\n","\n","from sklearn import svm\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier \n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, accuracy_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","\n","import nltk\n","from nltk.parse.corenlp import CoreNLPServer, CoreNLPParser, CoreNLPDependencyParser\n","from nltk.tokenize import sent_tokenize\n","from collections import Counter\n","\n","import stanfordnlp"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\ioana\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["nltk.download('averaged_perceptron_tagger')\n","\n","java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_321/bin/java.exe\"\n","os.environ['JAVAHOME'] = java_path\n","nltk.internals.config_java(java_path)\n","\n","CORENLP_JAR = os.path.join(\"models\", \"stanford-corenlp-4.5.4\", \"stanford-corenlp-4.5.4.jar\")"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[52], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m server \u001b[39m=\u001b[39m CoreNLPServer(\n\u001b[0;32m      5\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(STANFORD, \u001b[39m\"\u001b[39m\u001b[39mstanford-corenlp-4.5.4.jar\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(STANFORD, \u001b[39m\"\u001b[39m\u001b[39mstanford-corenlp-4.5.4-models.jar\u001b[39m\u001b[39m\"\u001b[39m),    \n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[39m# Start the server in the background\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m server\u001b[39m.\u001b[39;49mstart()\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\nltk\\parse\\corenlp.py:145\u001b[0m, in \u001b[0;36mCoreNLPServer.start\u001b[1;34m(self, stdout, stderr)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(requests\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49murljoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39m\"\u001b[39;49m\u001b[39mlive\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError:\n\u001b[0;32m    147\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n","File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n","File \u001b[1;32mc:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["STANFORD = os.path.join(\"models\", \"stanford-corenlp-4.5.4\")\n","\n","# Create the server\n","server = CoreNLPServer(\n","    os.path.join(STANFORD, \"stanford-corenlp-4.5.4.jar\"),\n","    os.path.join(STANFORD, \"stanford-corenlp-4.5.4-models.jar\"),    \n",")\n","\n","# Start the server in the background\n","server.start()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus size:  1224\n"]}],"source":["corpus_path = \"data\\\\formatted_corpus.json\"\n","with open(corpus_path, 'r', encoding='utf-8') as corpus_json:\n","    corpus = json.load(corpus_json)\n","print(\"Corpus size: \", len(corpus))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Visualize data"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["FKRE ratings: {'Fairly easy': 566, 'Plain English': 368, 'Easy': 178, 'Fairly difficult': 94, 'Very easy': 8, 'Difficult': 10}\n","FKRE scores: min 41.7, max 94.9, average 71.62647058823535\n","Words per minute: min 71.0, max 275.0, average 160.2393790849673\n","Words in the New Academic Word List: min 1, max 61, average 19.40767973856209\n","Words in the New General Service List: min 224, max 645, average 415.577614379085\n","Length in seconds: min 481, max 1199, average 884.9648692810457\n","Number of views: min 231850, max 75063761, average 2817607.3333333335\n","Number of likes: min 10000, max 22000000, average 175500.0\n","Number of likes per view: min 0.027548623320160084, max 0.2995794216171842, average 0.03349684151685497\n"]},{"data":{"text/plain":["<Figure size 5000x2000 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["FKRE_ratings = {}\n","FKRE_scores = []\n","WPMs = []\n","NAWLs = []\n","NGSLs = []\n","seconds = []\n","view_counts = []\n","like_counts = []\n","likes_per_view = []\n","for talk in corpus:\n","    if talk['FKRE_rating'] in FKRE_ratings:\n","        FKRE_ratings[talk['FKRE_rating']] += 1\n","    else:\n","        FKRE_ratings[talk['FKRE_rating']] = 1\n","    FKRE_scores.append(talk['FKRE_score'])\n","    WPMs.append(talk['WPM'])\n","    NAWLs.append(talk['NAWL'])\n","    NGSLs.append(talk['NGSL'])\n","    seconds.append(talk['seconds'])\n","    view_counts.append(talk['view_count'])\n","    like_counts.append(talk['like_count'])\n","    likes_per_view.append(talk['likes_per_view'])\n","\n","print(f'FKRE ratings: {FKRE_ratings}')\n","print(f'FKRE scores: min {min(FKRE_scores)}, max {max(FKRE_scores)}, average {sum(FKRE_scores) / len(FKRE_scores)}')\n","print(f'Words per minute: min {min(WPMs)}, max {max(WPMs)}, average {sum(WPMs) / len(WPMs)}')\n","print(f'Words in the New Academic Word List: min {min(NAWLs)}, max {max(NAWLs)}, average {sum(NAWLs) / len(NAWLs)}')\n","print(f'Words in the New General Service List: min {min(NGSLs)}, max {max(NGSLs)}, average {sum(NGSLs) / len(NGSLs)}')\n","print(f'Length in seconds: min {min(seconds)}, max {max(seconds)}, average {sum(seconds) / len(seconds)}')\n","print(f'Number of views: min {min(view_counts)}, max {max(view_counts)}, average {sum(view_counts) / len(view_counts)}')\n","print(f'Number of likes: min {min(like_counts)}, max {max(like_counts)}, average {sum(like_counts) / len(like_counts)}')\n","print(f'Number of likes per view: min {min(likes_per_view)}, max {max(likes_per_view)}, average {sum(likes_per_view) / len(likes_per_view)}')\n","\n","plt.figure(figsize=(50, 20))\n","\n","plt.bar(range(len(FKRE_scores)), FKRE_scores)\n","plt.title('FKRE scores')\n","plt.savefig(f'data/stats/FKRE_scores.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(WPMs)), WPMs)\n","plt.title('Words per minute')\n","plt.savefig(f'data/stats/WPMs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(NAWLs)), NAWLs)\n","plt.title('Words in the New Academic Word List')\n","plt.savefig(f'data/stats/NAWLs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(NGSLs)), NGSLs)\n","plt.title('Words in the New General Service List')\n","plt.savefig(f'data/stats/NGSLs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(seconds)), seconds)\n","plt.title('Length in seconds')\n","plt.savefig(f'data/stats/seconds.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(view_counts)), view_counts)\n","plt.title('Number of views')\n","plt.savefig(f'data/stats/view_counts.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(like_counts)), like_counts)\n","plt.title('Number of likes')\n","plt.savefig(f'data/stats/like_counts.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(likes_per_view)), likes_per_view)\n","plt.title('Number of likes per view')\n","plt.savefig(f'data/stats/likes_per_view.jpg')\n","plt.clf()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Normalize metrics"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n"]}],"source":["weights = np.array([4, 1, 1, 1, 1, 5, 3, 2, 1])\n","normalized_array = np.divide(weights, np.sum(weights))\n","print(sum(normalized_array))"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Cluster 2: Rank 1, 22 talks\n","Cluster 1: Rank 2, 171 talks\n","Cluster 3: Rank 3, 386 talks\n","Cluster 4: Rank 4, 411 talks\n","Cluster 0: Rank 5, 234 talks\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>FKRE_rating</th>\n","      <th>FKRE_score</th>\n","      <th>WPM</th>\n","      <th>NAWL</th>\n","      <th>NGSL</th>\n","      <th>URL</th>\n","      <th>seconds</th>\n","      <th>view_count</th>\n","      <th>like_count</th>\n","      <th>transcript</th>\n","      <th>raw_transcript</th>\n","      <th>likes_per_view</th>\n","      <th>total_responses</th>\n","      <th>laughter</th>\n","      <th>applause</th>\n","      <th>cheering</th>\n","      <th>popularity_score</th>\n","      <th>popularity_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Aaron Huey: America's native prisoners of war</td>\n","      <td>Fairly easy</td>\n","      <td>70.0</td>\n","      <td>146.0</td>\n","      <td>12</td>\n","      <td>458</td>\n","      <td>https://www.ted.com/talks/aaron_huey</td>\n","      <td>916</td>\n","      <td>1970692</td>\n","      <td>59000</td>\n","      <td>[{'sentence': 'I'm here today to show my photo...</td>\n","      <td>i'm here today to show my photographs of the l...</td>\n","      <td>0.029939</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.211961</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abha Dawesar: Life in the \"digital now\"</td>\n","      <td>Fairly easy</td>\n","      <td>74.7</td>\n","      <td>169.0</td>\n","      <td>16</td>\n","      <td>435</td>\n","      <td>https://www.ted.com/talks/abha_dawesar_life_in...</td>\n","      <td>713</td>\n","      <td>1369143</td>\n","      <td>41000</td>\n","      <td>[{'sentence': 'I was in New York during Hurric...</td>\n","      <td>i was in new york during hurricane sandy, and ...</td>\n","      <td>0.029946</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.222254</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Abraham Verghese: A doctor's touch</td>\n","      <td>Fairly easy</td>\n","      <td>70.1</td>\n","      <td>170.0</td>\n","      <td>41</td>\n","      <td>526</td>\n","      <td>https://www.ted.com/talks/abraham_verghese_a_d...</td>\n","      <td>1100</td>\n","      <td>1992577</td>\n","      <td>59000</td>\n","      <td>[{'sentence': 'A few months ago, a 40 year-old...</td>\n","      <td>a few months ago, a 40 year-old woman came to ...</td>\n","      <td>0.029610</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.265658</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Adam Davidson: What we learned from teetering ...</td>\n","      <td>Plain English</td>\n","      <td>61.1</td>\n","      <td>165.0</td>\n","      <td>24</td>\n","      <td>546</td>\n","      <td>https://www.ted.com/talks/adam_davidson_what_w...</td>\n","      <td>1177</td>\n","      <td>838052</td>\n","      <td>25000</td>\n","      <td>[{'sentence': 'So a friend of mine who's a pol...</td>\n","      <td>so a friend of mine who's a political scientis...</td>\n","      <td>0.029831</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.222199</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Adam Garone: Healthier men, one moustache at a...</td>\n","      <td>Fairly easy</td>\n","      <td>74.2</td>\n","      <td>171.0</td>\n","      <td>12</td>\n","      <td>416</td>\n","      <td>https://www.ted.com/talks/adam_garone_healthie...</td>\n","      <td>989</td>\n","      <td>755891</td>\n","      <td>22000</td>\n","      <td>[{'sentence': 'I think the beautiful Malin [Ak...</td>\n","      <td>i think the beautiful malin [akerman] put it p...</td>\n","      <td>0.029105</td>\n","      <td>{'laughter': 28, 'applause': 4, 'cheering': 0}</td>\n","      <td>28</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.318285</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1219</th>\n","      <td>Zahra' Langhi: Why Libya's revolution didn't w...</td>\n","      <td>Difficult</td>\n","      <td>48.2</td>\n","      <td>104.0</td>\n","      <td>18</td>\n","      <td>255</td>\n","      <td>https://www.ted.com/talks/zahra_langhi_why_lib...</td>\n","      <td>576</td>\n","      <td>561666</td>\n","      <td>16000</td>\n","      <td>[{'sentence': 'I have never, ever forgotten th...</td>\n","      <td>i have never, ever forgotten the words of my g...</td>\n","      <td>0.028487</td>\n","      <td>{'laughter': 0, 'applause': 4, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.090970</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1220</th>\n","      <td>Zainab Salbi: Women, wartime and the dream of ...</td>\n","      <td>Fairly easy</td>\n","      <td>74.1</td>\n","      <td>128.0</td>\n","      <td>14</td>\n","      <td>413</td>\n","      <td>https://www.ted.com/talks/zainab_salbi</td>\n","      <td>1054</td>\n","      <td>618890</td>\n","      <td>18000</td>\n","      <td>[{'sentence': 'I woke up in the middle of the ...</td>\n","      <td>i woke up in the middle of the night with the ...</td>\n","      <td>0.029084</td>\n","      <td>{'laughter': 1, 'applause': 1, 'cheering': 0}</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.231153</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1221</th>\n","      <td>Zak Ebrahim: I am the son of a terrorist. Here...</td>\n","      <td>Plain English</td>\n","      <td>67.6</td>\n","      <td>152.0</td>\n","      <td>11</td>\n","      <td>368</td>\n","      <td>https://www.ted.com/talks/zak_ebrahim_i_am_the...</td>\n","      <td>545</td>\n","      <td>6602165</td>\n","      <td>198000</td>\n","      <td>[{'sentence': 'On November 5th, 1990, a man na...</td>\n","      <td>on november 5th, 1990, a man named el-sayyid n...</td>\n","      <td>0.029990</td>\n","      <td>{'laughter': 0, 'applause': 4, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.187294</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1222</th>\n","      <td>Zeresenay Alemseged: The search for humanity's...</td>\n","      <td>Fairly easy</td>\n","      <td>71.9</td>\n","      <td>160.0</td>\n","      <td>19</td>\n","      <td>456</td>\n","      <td>https://www.ted.com/talks/zeresenay_alemseged_...</td>\n","      <td>943</td>\n","      <td>1228159</td>\n","      <td>36000</td>\n","      <td>[{'sentence': 'I have 18 minutes to tell you w...</td>\n","      <td>i have 18 minutes to tell you what happened ov...</td>\n","      <td>0.029312</td>\n","      <td>{'laughter': 4, 'applause': 1, 'cheering': 0}</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.239127</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1223</th>\n","      <td>Zeynep Tufekci: Online social change: easy to ...</td>\n","      <td>Fairly difficult</td>\n","      <td>58.7</td>\n","      <td>143.0</td>\n","      <td>18</td>\n","      <td>466</td>\n","      <td>https://www.ted.com/talks/zeynep_tufekci_how_t...</td>\n","      <td>967</td>\n","      <td>1407711</td>\n","      <td>42000</td>\n","      <td>[{'sentence': 'So recently, we heard a lot abo...</td>\n","      <td>so recently, we heard a lot about how social m...</td>\n","      <td>0.029836</td>\n","      <td>{'laughter': 1, 'applause': 1, 'cheering': 0}</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.178565</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1224 rows Ã— 19 columns</p>\n","</div>"],"text/plain":["                                                  title       FKRE_rating   \n","0         Aaron Huey: America's native prisoners of war       Fairly easy  \\\n","1               Abha Dawesar: Life in the \"digital now\"       Fairly easy   \n","2                    Abraham Verghese: A doctor's touch       Fairly easy   \n","3     Adam Davidson: What we learned from teetering ...     Plain English   \n","4     Adam Garone: Healthier men, one moustache at a...       Fairly easy   \n","...                                                 ...               ...   \n","1219  Zahra' Langhi: Why Libya's revolution didn't w...         Difficult   \n","1220  Zainab Salbi: Women, wartime and the dream of ...       Fairly easy   \n","1221  Zak Ebrahim: I am the son of a terrorist. Here...     Plain English   \n","1222  Zeresenay Alemseged: The search for humanity's...       Fairly easy   \n","1223  Zeynep Tufekci: Online social change: easy to ...  Fairly difficult   \n","\n","      FKRE_score    WPM  NAWL  NGSL   \n","0           70.0  146.0    12   458  \\\n","1           74.7  169.0    16   435   \n","2           70.1  170.0    41   526   \n","3           61.1  165.0    24   546   \n","4           74.2  171.0    12   416   \n","...          ...    ...   ...   ...   \n","1219        48.2  104.0    18   255   \n","1220        74.1  128.0    14   413   \n","1221        67.6  152.0    11   368   \n","1222        71.9  160.0    19   456   \n","1223        58.7  143.0    18   466   \n","\n","                                                    URL  seconds  view_count   \n","0                  https://www.ted.com/talks/aaron_huey      916     1970692  \\\n","1     https://www.ted.com/talks/abha_dawesar_life_in...      713     1369143   \n","2     https://www.ted.com/talks/abraham_verghese_a_d...     1100     1992577   \n","3     https://www.ted.com/talks/adam_davidson_what_w...     1177      838052   \n","4     https://www.ted.com/talks/adam_garone_healthie...      989      755891   \n","...                                                 ...      ...         ...   \n","1219  https://www.ted.com/talks/zahra_langhi_why_lib...      576      561666   \n","1220             https://www.ted.com/talks/zainab_salbi     1054      618890   \n","1221  https://www.ted.com/talks/zak_ebrahim_i_am_the...      545     6602165   \n","1222  https://www.ted.com/talks/zeresenay_alemseged_...      943     1228159   \n","1223  https://www.ted.com/talks/zeynep_tufekci_how_t...      967     1407711   \n","\n","      like_count                                         transcript   \n","0          59000  [{'sentence': 'I'm here today to show my photo...  \\\n","1          41000  [{'sentence': 'I was in New York during Hurric...   \n","2          59000  [{'sentence': 'A few months ago, a 40 year-old...   \n","3          25000  [{'sentence': 'So a friend of mine who's a pol...   \n","4          22000  [{'sentence': 'I think the beautiful Malin [Ak...   \n","...          ...                                                ...   \n","1219       16000  [{'sentence': 'I have never, ever forgotten th...   \n","1220       18000  [{'sentence': 'I woke up in the middle of the ...   \n","1221      198000  [{'sentence': 'On November 5th, 1990, a man na...   \n","1222       36000  [{'sentence': 'I have 18 minutes to tell you w...   \n","1223       42000  [{'sentence': 'So recently, we heard a lot abo...   \n","\n","                                         raw_transcript  likes_per_view   \n","0     i'm here today to show my photographs of the l...        0.029939  \\\n","1     i was in new york during hurricane sandy, and ...        0.029946   \n","2     a few months ago, a 40 year-old woman came to ...        0.029610   \n","3     so a friend of mine who's a political scientis...        0.029831   \n","4     i think the beautiful malin [akerman] put it p...        0.029105   \n","...                                                 ...             ...   \n","1219  i have never, ever forgotten the words of my g...        0.028487   \n","1220  i woke up in the middle of the night with the ...        0.029084   \n","1221  on november 5th, 1990, a man named el-sayyid n...        0.029990   \n","1222  i have 18 minutes to tell you what happened ov...        0.029312   \n","1223  so recently, we heard a lot about how social m...        0.029836   \n","\n","                                     total_responses  laughter  applause   \n","0      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1  \\\n","1      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","2      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","3      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","4     {'laughter': 28, 'applause': 4, 'cheering': 0}        28         4   \n","...                                              ...       ...       ...   \n","1219   {'laughter': 0, 'applause': 4, 'cheering': 0}         0         4   \n","1220   {'laughter': 1, 'applause': 1, 'cheering': 0}         1         1   \n","1221   {'laughter': 0, 'applause': 4, 'cheering': 0}         0         4   \n","1222   {'laughter': 4, 'applause': 1, 'cheering': 0}         4         1   \n","1223   {'laughter': 1, 'applause': 1, 'cheering': 0}         1         1   \n","\n","      cheering  popularity_score  popularity_category  \n","0            0          0.211961                    4  \n","1            0          0.222254                    4  \n","2            0          0.265658                    3  \n","3            0          0.222199                    4  \n","4            0          0.318285                    1  \n","...        ...               ...                  ...  \n","1219         0          0.090970                    0  \n","1220         0          0.231153                    4  \n","1221         0          0.187294                    0  \n","1222         0          0.239127                    4  \n","1223         0          0.178565                    0  \n","\n","[1224 rows x 19 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["data = pd.DataFrame(corpus)\n","\n","# Extract laughter, applause, and cheering counts from the \"total_responses\" dictionary\n","data['laughter'] = data['total_responses'].apply(lambda x: x['laughter'])\n","data['applause'] = data['total_responses'].apply(lambda x: x['applause'])\n","data['cheering'] = data['total_responses'].apply(lambda x: x['cheering'])\n","\n","# Normalize the metrics\n","scaler = MinMaxScaler()\n","normalized_data = scaler.fit_transform(data[['FKRE_score', 'WPM', 'NAWL', 'NGSL', 'seconds', 'likes_per_view', 'laughter', 'applause', 'cheering']])\n","weights = np.array([4, 1, 1, 1, 1, 5, 3, 2, 1])\n","weights = np.divide(weights, np.sum(weights))\n","\n","# Calculate the popularity score\n","popularity_score = np.dot(normalized_data, weights)\n","\n","# Add the popularity score to the DataFrame\n","data['popularity_score'] = popularity_score\n","\n","# Classify the talks using K-means clustering\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","data['popularity_category'] = kmeans.fit_predict(data[['popularity_score']])\n","\n","# Calculate the average popularity score for each cluster\n","cluster_popularity = data.groupby('popularity_category')['popularity_score'].mean()\n","\n","# Sort the clusters by popularity score in descending order\n","sorted_clusters = cluster_popularity.sort_values(ascending=False)\n","\n","# Count the number of talks in each cluster\n","talks_per_cluster = data['popularity_category'].value_counts()\n","\n","# Print the ranking of the clusters by popularity score\n","for i, cluster in enumerate(sorted_clusters.index):\n","    print(f\"Cluster {cluster}: Rank {i+1}, {talks_per_cluster[cluster]} talks\")\n","\n","display(data)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"ename":"CalledProcessError","evalue":"Command '['java', '-cp', 'models\\\\stanford-corenlp-4.5.4\\\\stanford-corenlp-4.5.4.jar', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize', '-outputFormat', 'json', '-file', '-', '-']' returned non-zero exit status 1.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mcheck_output([\u001b[39m'\u001b[39;49m\u001b[39mjava\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-cp\u001b[39;49m\u001b[39m'\u001b[39;49m, CORENLP_JAR, \u001b[39m'\u001b[39;49m\u001b[39medu.stanford.nlp.pipeline.StanfordCoreNLP\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-annotators\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtokenize\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-outputFormat\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-file\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mThis is a test sentence.\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mencode())\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(output)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:421\u001b[0m, in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m         empty \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    419\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[1;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39mpopenargs, stdout\u001b[39m=\u001b[39mPIPE, timeout\u001b[39m=\u001b[39mtimeout, check\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m            \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mstdout\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n\u001b[0;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[1;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[0;32m    527\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n","\u001b[1;31mCalledProcessError\u001b[0m: Command '['java', '-cp', 'models\\\\stanford-corenlp-4.5.4\\\\stanford-corenlp-4.5.4.jar', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize', '-outputFormat', 'json', '-file', '-', '-']' returned non-zero exit status 1."]}],"source":["#ciorna\n","#TODO: get it to work\n","\n","output = subprocess.check_output(['java', '-cp', CORENLP_JAR, 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize', '-outputFormat', 'json', '-file', '-', '-'], input='This is a test sentence.'.encode())\n","print(output)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use NLP techniques to extract extra features (WIP)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"ename":"CalledProcessError","evalue":"Command '['java', '-cp', 'models\\\\stanford-corenlp-4.5.4\\\\stanford-corenlp-4.5.4.jar', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit,pos,parse', '-outputFormat', 'json', '-file', '-', '-']' returned non-zero exit status 1.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[1;32mIn[62], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m window_sentences \u001b[39m=\u001b[39m sentences[start:end]\n\u001b[0;32m     74\u001b[0m \u001b[39m# Compute the features for the current window\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m features \u001b[39m=\u001b[39m compute_features(window_sentences)\n\u001b[0;32m     76\u001b[0m features_list\u001b[39m.\u001b[39mappend(features)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Move the window one sentence forward\u001b[39;00m\n","Cell \u001b[1;32mIn[62], line 11\u001b[0m, in \u001b[0;36mcompute_features\u001b[1;34m(window_sentences)\u001b[0m\n\u001b[0;32m      8\u001b[0m num_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(window_sentences)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Compute the mean length of clause\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mcheck_output([\u001b[39m'\u001b[39;49m\u001b[39mjava\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-cp\u001b[39;49m\u001b[39m'\u001b[39;49m, CORENLP_JAR, \u001b[39m'\u001b[39;49m\u001b[39medu.stanford.nlp.pipeline.StanfordCoreNLP\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-annotators\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtokenize,ssplit,pos,parse\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-outputFormat\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-file\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mwindow_text\u001b[39m.\u001b[39;49mencode())\n\u001b[0;32m     12\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m     13\u001b[0m parse_trees \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparse\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(.*?)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, output)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:421\u001b[0m, in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m         empty \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    419\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[1;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39mpopenargs, stdout\u001b[39m=\u001b[39mPIPE, timeout\u001b[39m=\u001b[39mtimeout, check\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m            \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mstdout\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n\u001b[0;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[1;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[0;32m    527\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n","\u001b[1;31mCalledProcessError\u001b[0m: Command '['java', '-cp', 'models\\\\stanford-corenlp-4.5.4\\\\stanford-corenlp-4.5.4.jar', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit,pos,parse', '-outputFormat', 'json', '-file', '-', '-']' returned non-zero exit status 1."]}],"source":["def compute_features(window_sentences):\n","    \"\"\"Compute the features for a given window\"\"\"\n","\n","    # Combine all sentences in the window\n","    window_text = ' '.join(window_sentences)\n","    \n","    # Compute the number of sentences in the window\n","    num_sentences = len(window_sentences)\n","    \n","    # Compute the mean length of clause\n","    output = subprocess.check_output(['java', '-cp', CORENLP_JAR, 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit,pos,parse', '-outputFormat', 'json', '-file', '-', '-'], input=window_text.encode())\n","    output = output.decode()\n","    parse_trees = re.findall(r'\"parse\": \"(.*?)\"', output)\n","    clauses = []\n","    for parse_tree in parse_trees:\n","        clauses.extend(re.findall(r'\\(S .*?\\)', parse_tree))\n","    clause_lengths = [len(re.findall(r'\\(VB.*?\\)', clause)) + 1 for clause in clauses]\n","    mean_clause_length = sum(clause_lengths) / len(clause_lengths)\n","    \n","    # Compute clauses per sentence\n","    clauses_per_sentence = len(clauses) / num_sentences\n","    \n","    # Compute coordinate phrases per clause\n","    coordinate_phrases = []\n","    for parse_tree in parse_trees:\n","        coordinate_phrases.extend(re.findall(r'\\(CC .*?\\)', parse_tree))\n","    coordinate_phrases_per_clause = len(coordinate_phrases) / len(clauses)\n","    \n","    # Compute complex nominals per clause\n","    complex_nominals = []\n","    for parse_tree in parse_trees:\n","        complex_nominals.extend(re.findall(r'\\(NP .*?SBAR', parse_tree))\n","    complex_nominals_per_clause = len(complex_nominals) / len(clauses)\n","    \n","    # Compute type-token ratio\n","    words = window_text.split()\n","    type_token_ratio = len(set(words)) / len(words)\n","    \n","    # Compute n-grams frequency features\n","    n_grams = [2, 3, 4, 5]\n","    n_grams_freq = {}\n","    for n in n_grams:\n","        n_grams_freq[f\"{n}-\"] = Counter(ngrams(words, n))\n","    \n","    return {\n","        'mean_clause_length': mean_clause_length,\n","        'clauses_per_sentence': clauses_per_sentence,\n","        'coordinate_phrases_per_clause': coordinate_phrases_per_clause,\n","        'complex_nominals_per_clause': complex_nominals_per_clause,\n","        'type_token_ratio': type_token_ratio,\n","        'n_grams_freq': n_grams_freq\n","    }\n","\n","# Define the sliding window size\n","window_size = 5\n","window_features = []\n","\n","# Iterate through the raw_transcript column\n","for index, row in data.iterrows():\n","    transcript = row['raw_transcript']\n","    \n","    # Tokenize the transcript into sentences\n","    sentences = sent_tokenize(transcript)\n","\n","    # Initialize the window start and end indices\n","    start = 0\n","    end = window_size\n","\n","    # Iterate through the sentences using the sliding window\n","    features_list = []\n","    while end <= len(sentences):\n","        # Extract the sentences in the current window\n","        window_sentences = sentences[start:end]\n","\n","        # Compute the features for the current window\n","        features = compute_features(window_sentences)\n","        features_list.append(features)\n","\n","        # Move the window one sentence forward\n","        start += 1\n","        end += 1\n","\n","    # Store the features in the DataFrame\n","    window_features.append(features_list)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of talks in training set: 976\n","Number of talks in testing set: 248\n"]}],"source":["features = ['FKRE_score', 'WPM', 'NAWL', 'NGSL', 'seconds', 'likes_per_view', 'laughter', 'applause', 'cheering', 'popularity_score']\n","\n","# Create a list of cluster labels and corresponding number of talks\n","cluster_labels = []\n","num_talks = []\n","for cluster, talks_nr in talks_per_cluster.items():\n","    cluster_labels.append(cluster)\n","    num_talks.append(talks_nr)\n","\n","# Create a list of indices for each cluster\n","cluster_indices = [np.where(data['popularity_category'] == label)[0] for label in cluster_labels]\n","\n","# Split each cluster into training and testing sets\n","train_indices = []\n","test_indices = []\n","for indices in cluster_indices:\n","    # Select the features and target variable for this cluster\n","    X = data.loc[indices, features]\n","    y = data.loc[indices, 'popularity_category']\n","\n","    # Normalize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Add the indices to the training and testing sets\n","    train_indices.extend(indices[X_train.astype(int)])\n","    test_indices.extend(indices[X_test.astype(int)])\n","\n","# Print the number of talks in the training and testing sets\n","print(f\"Number of talks in training set: {len(train_indices)}\")\n","print(f\"Number of talks in testing set: {len(test_indices)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["How to run CoreNLP Server\n","\n","cd .\\models\\stanford-corenlp-4.5.4\n","java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8080 -timeout 50000\n","\n","CoreNLP paper:\n","Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["more ciorna"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'CoreNLPDependencyParser' object has no attribute 'tregex'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[44], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m     63\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThank you. It\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms really great to be here. I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm going to talk to you today about something that\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms very important to me, and that\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms the power of education.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 64\u001b[0m features \u001b[39m=\u001b[39m extract_cocogen_features(text, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     65\u001b[0m \u001b[39mprint\u001b[39m(features)\n","Cell \u001b[1;32mIn[44], line 53\u001b[0m, in \u001b[0;36mextract_cocogen_features\u001b[1;34m(text, ws)\u001b[0m\n\u001b[0;32m     50\u001b[0m parse_trees \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_sents(window)\n\u001b[0;32m     52\u001b[0m syntactic_counts \u001b[39m=\u001b[39m compute_syntactic_features(parse_trees)\n\u001b[1;32m---> 53\u001b[0m lexical_counts \u001b[39m=\u001b[39m compute_lexical_features(window)\n\u001b[0;32m     54\u001b[0m ngram_counts \u001b[39m=\u001b[39m compute_ngram_features(window)\n\u001b[0;32m     56\u001b[0m features\u001b[39m.\u001b[39mextend(syntactic_counts\u001b[39m.\u001b[39mvalues())\n","Cell \u001b[1;32mIn[44], line 27\u001b[0m, in \u001b[0;36mcompute_lexical_features\u001b[1;34m(window)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m feature\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mLS\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(feature\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39mLS\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 27\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dep_parser\u001b[39m.\u001b[39;49mtregex(sentence, pattern))\n\u001b[0;32m     28\u001b[0m     lexical_counts[feature] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(matches)\n\u001b[0;32m     29\u001b[0m \u001b[39melse\u001b[39;00m:\n","\u001b[1;31mAttributeError\u001b[0m: 'CoreNLPDependencyParser' object has no attribute 'tregex'"]}],"source":["# Load the Stanford CoreNLP parser\n","parser = CoreNLPParser(url='http://localhost:8080')\n","dep_parser = CoreNLPDependencyParser(url='http://localhost:8080')\n","\n","# Define the feature groups\n","syntactic_features = ['NP', 'VP', 'PP', 'SBAR', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NX', 'PRN', 'PRT', 'QP', 'RRC', 'UCP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP']\n","lexical_features = ['LV', 'LS', 'LS1', 'LS2', 'LS3', 'LD', 'LSO1', 'LSO2', 'LSO3', 'LSS1', 'LSS', 'LSS3']\n","ngram_features = ['spoken', 'magazine', 'fiction', 'news', 'academic']\n","\n","def compute_syntactic_features(parse_trees):\n","    syntactic_counts = Counter()\n","    for tree_iterator in parse_trees:\n","        for tree in tree_iterator:\n","            for subtree in tree.subtrees():\n","                if subtree.label() in syntactic_features:\n","                    syntactic_counts[subtree.label()] += 1\n","    return syntactic_counts\n","\n","def compute_lexical_features(window):\n","    lexical_counts = Counter()\n","    for sentence in window:\n","        tokens = nltk.word_tokenize(sentence)\n","        pos_tags = nltk.pos_tag(tokens)\n","        for feature in lexical_features:\n","            if feature.startswith('LS'):\n","                pattern = ' '.join(feature.split('LS')[1].split('_'))\n","                matches = list(dep_parser.tregex(sentence, pattern))\n","                lexical_counts[feature] += len(matches)\n","            else:\n","                freq_dist = nltk.FreqDist([token.lower() for token, pos in pos_tags if pos.startswith(feature)])\n","                lexical_counts[feature] += freq_dist.B()\n","    return lexical_counts\n","\n","def compute_ngram_features(window):\n","    ngram_counts = Counter()\n","    for feature in ngram_features:\n","        for sentence in window:\n","            tokens = nltk.word_tokenize(sentence)\n","            ngrams = nltk.ngrams(tokens, n=2)\n","            freq_dist = nltk.FreqDist([ngram for ngram in ngrams if ngram[0].lower() in feature.split('_')])\n","            ngram_counts[feature] += freq_dist.B()\n","    return ngram_counts\n","\n","def extract_cocogen_features(text, ws=10):\n","    sentences = sent_tokenize(text)\n","    features = []\n","\n","    for i in range(len(sentences) - ws + 1):\n","        window = sentences[i:i+ws]\n","        parse_trees = parser.parse_sents(window)\n","\n","        syntactic_counts = compute_syntactic_features(parse_trees)\n","        lexical_counts = compute_lexical_features(window)\n","        ngram_counts = compute_ngram_features(window)\n","\n","        features.extend(syntactic_counts.values())\n","        features.extend(lexical_counts.values())\n","        features.extend(ngram_counts.values())\n","\n","    return features\n","\n","# Example usage\n","text = \"Thank you. It's really great to be here. I'm going to talk to you today about something that's very important to me, and that's the power of education.\"\n","features = extract_cocogen_features(text, 2)\n","print(features)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["scaler = MinMaxScaler (feature_range = (0,1))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape:  (1224, 5, 1)\n","y shape:  (1224, 1, 1)\n","[[0.00753109]\n"," [0.98230349]]\n","[0 0 0 ... 0 0 0]\n","Good examples:  (1206, 5, 1) (1206, 1, 1)\n","Bad examples:  (18, 5, 1) (18, 1, 1)\n","Train corpus size:  978\n","Test corpus size:  246\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\ioana\\Documents\\FACULTATE\\Master\\SSL&NLP\\NLP_Project_2023\\.venv310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]}],"source":["def score(item):\n","    reactions = {'applause': 3, 'laughter': 2, 'cheering': 1}\n","    total_score = 0\n","    for key in reactions:\n","        total_score += item['total_responses'][key] * reactions[key]\n","    return total_score\n","\n","\n","def extract_features_labels(corpus):\n","    X = []\n","    y = []\n","    for data in corpus:\n","        features = []\n","        features.append(data[\"FKRE_score\"])\n","        # features.append(data[\"seconds\"])\n","        features.append(data[\"NAWL\"])\n","        features.append(data[\"NGSL\"])\n","        features.append(data[\"WPM\"])\n","        features.append(score(data))\n","        X.append(features)\n","        # Like count to view count ratio\n","        y.append(data[\"like_count\"]/data[\"view_count\"])\n","    X = np.array(X)\n","    y = np.array(y)\n","\n","    X = scaler.fit_transform(X)\n","    y = scaler.fit_transform(y.reshape(-1,1))\n","\n","    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n","    y = np.reshape(y, (y.shape[0], y.shape[1], 1))\n","\n","    print(\"X shape: \", X.shape)\n","    print(\"y shape: \", y.shape)\n","\n","    return X, y\n","\n","X, y = extract_features_labels(corpus)\n","\n","# Perform KMeans clustering with 2 clusters\n","kmeans = KMeans(n_clusters=2, random_state=0)\n","kmeans.fit(np.reshape(y, (y.shape[0], y.shape[1])))\n","\n","# Get the cluster centroids\n","print(kmeans.cluster_centers_)\n","# Get the cluster labels\n","print(kmeans.labels_)\n","\n","# Get the good and bad examples\n","good_examples_X = X[kmeans.labels_ == 0]\n","bad_examples_X = X[kmeans.labels_ == 1]\n","\n","good_examples_y = y[kmeans.labels_ == 0]\n","bad_examples_y = y[kmeans.labels_ == 1]\n","\n","print(\"Good examples: \", good_examples_X.shape, good_examples_y.shape)\n","print(\"Bad examples: \", bad_examples_X.shape, bad_examples_y.shape)\n","\n","# Split the corpus into train and test sets\n","X_train_good, X_test_good, y_train_good, y_test_good = train_test_split(good_examples_X, good_examples_y, test_size=0.2, shuffle=False)\n","X_train_bad, X_test_bad, y_train_bad, y_test_bad = train_test_split(bad_examples_X, bad_examples_y, test_size=0.2, shuffle=False)\n","\n","# Concatenate the good and bad examples\n","X_train = np.concatenate((X_train_good, X_train_bad))\n","X_test = np.concatenate((X_test_good, X_test_bad))\n","y_train = np.concatenate((y_train_good, y_train_bad))\n","y_test = np.concatenate((y_test_good, y_test_bad))\n","\n","print(\"Train corpus size: \", len(X_train))\n","print(\"Test corpus size: \", len(X_test))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","31/31 [==============================] - 7s 9ms/step - loss: 0.0136\n","Epoch 2/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 3/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 4/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0137\n","Epoch 5/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 6/30\n","31/31 [==============================] - 0s 10ms/step - loss: 0.0135\n","Epoch 7/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 8/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0134\n","Epoch 9/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0136\n","Epoch 10/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 11/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 12/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 13/30\n","31/31 [==============================] - 0s 10ms/step - loss: 0.0135\n","Epoch 14/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 15/30\n","31/31 [==============================] - 0s 10ms/step - loss: 0.0135\n","Epoch 16/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 17/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 18/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 19/30\n","31/31 [==============================] - 0s 10ms/step - loss: 0.0134\n","Epoch 20/30\n","31/31 [==============================] - 0s 10ms/step - loss: 0.0135\n","Epoch 21/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0136\n","Epoch 22/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0134\n","Epoch 23/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0134\n","Epoch 24/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 25/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 26/30\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0134\n","Epoch 27/30\n","31/31 [==============================] - 0s 9ms/step - loss: 0.0135\n","Epoch 28/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0134\n","Epoch 29/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n","Epoch 30/30\n","31/31 [==============================] - 0s 8ms/step - loss: 0.0135\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x2178038ed40>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["regressor = Sequential ()\n","# TO DO Text vectorization\n","# TO DO Embedding layer\n","regressor.add(LSTM(units = 50, return_sequences= True, input_shape = (X_train.shape[1], 1)))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50, return_sequences= True))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50, return_sequences= True))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50))\n","regressor.add(Dropout (0.2))\n","regressor.add(Dense (units=1))\n","\n","regressor.compile(optimizer='adam', loss='mean_squared_error')\n","regressor.fit(X_train, y_train, epochs=30, batch_size=32)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8/8 [==============================] - 1s 3ms/step\n","Predicted:  0.017070211 Actual:  0.008906125406766371\n","Predicted:  0.017108977 Actual:  0.008252673949505407\n","Predicted:  0.01704553 Actual:  0.008651126644492912\n","Predicted:  0.017022764 Actual:  0.0064495105534462666\n","Predicted:  0.017080953 Actual:  0.007424899921620112\n","Predicted:  0.017112933 Actual:  0.0077193602609216955\n","Predicted:  0.0170466 Actual:  0.008633628298997859\n","Predicted:  0.017027833 Actual:  0.008969998704320747\n","Predicted:  0.017040895 Actual:  0.008489957294764636\n","Predicted:  0.017040372 Actual:  0.004763138820858839\n","Predicted:  0.017091447 Actual:  0.00782552300522349\n","Predicted:  0.01701183 Actual:  0.008905487497858022\n","Predicted:  0.01699476 Actual:  0.008366034950310963\n","Predicted:  0.017102268 Actual:  0.008860057944798841\n","Predicted:  0.01707813 Actual:  0.007162274583595141\n","Predicted:  0.017107287 Actual:  0.008332371504353647\n","Predicted:  0.017022897 Actual:  0.008920918040276754\n","Predicted:  0.017042425 Actual:  0.005503781981377132\n","Predicted:  0.01707918 Actual:  0.007770788443306739\n","Predicted:  0.016989708 Actual:  0.008458736103629752\n","Predicted:  0.017007522 Actual:  0.007910004115133762\n","Predicted:  0.01703693 Actual:  0.007708676747901808\n","Predicted:  0.017079292 Actual:  0.007234964474981689\n","Predicted:  0.017043382 Actual:  0.007419928363175482\n","Predicted:  0.017097935 Actual:  0.00554311580683621\n","Predicted:  0.01712631 Actual:  0.0086330716019494\n","Predicted:  0.01699305 Actual:  0.00843243804697344\n","Predicted:  0.01705911 Actual:  0.008479431871837528\n","Predicted:  0.017010123 Actual:  0.005665082254627446\n","Predicted:  0.017066361 Actual:  0.008781031757629357\n","Predicted:  0.01705607 Actual:  0.00842980418709234\n","Predicted:  0.017085515 Actual:  0.008899349099953602\n","Predicted:  0.01707938 Actual:  0.006609628514540264\n","Predicted:  0.017082334 Actual:  0.008128900100773537\n","Predicted:  0.017152786 Actual:  0.007040809811109633\n","Predicted:  0.017119806 Actual:  0.006185133469302037\n","Predicted:  0.017060691 Actual:  0.008647450664849649\n","Predicted:  0.017072104 Actual:  0.00884840438489401\n","Predicted:  0.017106017 Actual:  0.008878367680974955\n","Predicted:  0.017101645 Actual:  0.0027794424335497586\n","Predicted:  0.017075107 Actual:  0.0071699053972466426\n","Predicted:  0.017025847 Actual:  0.00843255994401966\n","Predicted:  0.017048992 Actual:  0.008296303673167402\n","Predicted:  0.017057262 Actual:  0.005735986912500796\n","Predicted:  0.017076654 Actual:  0.008181820289121636\n","Predicted:  0.017043242 Actual:  0.00874932005460173\n","Predicted:  0.017050048 Actual:  0.004656023859257466\n","Predicted:  0.017013729 Actual:  0.008290302961143484\n","Predicted:  0.017097734 Actual:  0.00899052918316208\n","Predicted:  0.017009966 Actual:  0.008888771339067375\n","Predicted:  0.01704102 Actual:  0.007278414693959498\n","Predicted:  0.017028488 Actual:  0.00785179050411848\n","Predicted:  0.017012412 Actual:  0.008026462504529167\n","Predicted:  0.017017543 Actual:  0.00789807561541038\n","Predicted:  0.017094895 Actual:  0.008862095769558112\n","Predicted:  0.017082749 Actual:  0.00577431320281388\n","Predicted:  0.017070092 Actual:  0.00837549086453504\n","Predicted:  0.017021623 Actual:  0.0067277902286349\n","Predicted:  0.017069466 Actual:  0.007889806574440869\n","Predicted:  0.017089248 Actual:  0.00787066361745263\n","Predicted:  0.016999414 Actual:  0.008447042874375546\n","Predicted:  0.017044045 Actual:  0.008035936989684273\n","Predicted:  0.017039722 Actual:  0.005301369860965938\n","Predicted:  0.01700468 Actual:  0.005702577230600761\n","Predicted:  0.01709544 Actual:  0.007547791434928439\n","Predicted:  0.017136836 Actual:  0.008370027997004578\n","Predicted:  0.017074183 Actual:  0.0049713793932191774\n","Predicted:  0.017014911 Actual:  0.008118562533743351\n","Predicted:  0.017110627 Actual:  0.008854583016628018\n","Predicted:  0.017091831 Actual:  0.007751272986673913\n","Predicted:  0.017052036 Actual:  0.008755296156002138\n","Predicted:  0.016990779 Actual:  0.0075876939956689565\n","Predicted:  0.017008845 Actual:  0.008995583356930403\n","Predicted:  0.017036356 Actual:  0.006852253972372435\n","Predicted:  0.017083833 Actual:  0.0017513865395200195\n","Predicted:  0.01702378 Actual:  0.008774677298706068\n","Predicted:  0.017067712 Actual:  0.008756753187950148\n","Predicted:  0.017092416 Actual:  0.0033755221621645515\n","Predicted:  0.017024029 Actual:  0.008699556236662548\n","Predicted:  0.017044475 Actual:  0.00874001120383655\n","Predicted:  0.017026106 Actual:  0.008564021636437102\n","Predicted:  0.017086834 Actual:  0.008372081304086551\n","Predicted:  0.017052073 Actual:  0.007058618576046075\n","Predicted:  0.017043125 Actual:  0.007463878539594557\n","Predicted:  0.017060118 Actual:  0.0014260855597952315\n","Predicted:  0.01710898 Actual:  0.006363139277966848\n","Predicted:  0.017087944 Actual:  0.007797397203724757\n","Predicted:  0.017044699 Actual:  0.008400427390059043\n","Predicted:  0.017081615 Actual:  0.007462608982264543\n","Predicted:  0.017007172 Actual:  0.008714544987053333\n","Predicted:  0.017041957 Actual:  0.00686853759007186\n","Predicted:  0.017032864 Actual:  0.008033873591773572\n","Predicted:  0.01702673 Actual:  0.008975834633008009\n","Predicted:  0.01699734 Actual:  0.007489482903714678\n","Predicted:  0.017070755 Actual:  0.008886077803481204\n","Predicted:  0.017076198 Actual:  0.007143426496646607\n","Predicted:  0.017099366 Actual:  0.007780459396797626\n","Predicted:  0.017042575 Actual:  0.008482550109686057\n","Predicted:  0.017034365 Actual:  0.008939806007046316\n","Predicted:  0.01705639 Actual:  0.007701822195497754\n","Predicted:  0.017039062 Actual:  0.00851954624204182\n","Predicted:  0.017076626 Actual:  0.007708373003252916\n","Predicted:  0.017078426 Actual:  0.008078446568698869\n","Predicted:  0.017051175 Actual:  0.008560280042264154\n","Predicted:  0.017092146 Actual:  0.007096746353862413\n","Predicted:  0.017078582 Actual:  0.008230652572526476\n","Predicted:  0.017078176 Actual:  0.00827284944698678\n","Predicted:  0.017109137 Actual:  0.008662646987582595\n","Predicted:  0.017090293 Actual:  0.006761125197945819\n","Predicted:  0.01705116 Actual:  0.008282783126346172\n","Predicted:  0.017025506 Actual:  0.0052602555380152904\n","Predicted:  0.017089952 Actual:  0.007124211602633573\n","Predicted:  0.01712522 Actual:  0.008803666022327558\n","Predicted:  0.01703509 Actual:  0.0014808460972985937\n","Predicted:  0.01702502 Actual:  0.008775321180791731\n","Predicted:  0.01701024 Actual:  0.008384879407093648\n","Predicted:  0.017057776 Actual:  0.006192008785492489\n","Predicted:  0.017038602 Actual:  0.008534839880545972\n","Predicted:  0.017023431 Actual:  0.008564549391476994\n","Predicted:  0.01705018 Actual:  0.007471383639641088\n","Predicted:  0.017008465 Actual:  0.007836884466259741\n","Predicted:  0.017076325 Actual:  0.00608602175608465\n","Predicted:  0.01707492 Actual:  0.007564866555960614\n","Predicted:  0.017002046 Actual:  0.003597093649245972\n","Predicted:  0.017007828 Actual:  0.008871003168083563\n","Predicted:  0.017012257 Actual:  0.007597373759098691\n","Predicted:  0.017009534 Actual:  0.008925032306196401\n","Predicted:  0.016989395 Actual:  0.008101807616464354\n","Predicted:  0.017005265 Actual:  0.007714152725932039\n","Predicted:  0.017095814 Actual:  0.0035392885993275197\n","Predicted:  0.01704957 Actual:  0.008767267849139601\n","Predicted:  0.017022673 Actual:  0.007568754848161274\n","Predicted:  0.017027654 Actual:  0.008149569849062246\n","Predicted:  0.01697891 Actual:  0.008134257773965411\n","Predicted:  0.017105248 Actual:  0.008860400353209993\n","Predicted:  0.017107937 Actual:  0.00897496889678373\n","Predicted:  0.017025998 Actual:  0.007435356922991673\n","Predicted:  0.017015114 Actual:  0.008928127225885929\n","Predicted:  0.01710294 Actual:  0.008988289739666797\n","Predicted:  0.017079651 Actual:  0.00868722397586473\n","Predicted:  0.017068844 Actual:  0.004589917722077591\n","Predicted:  0.017071422 Actual:  0.007478782732611766\n","Predicted:  0.017062958 Actual:  0.008227821675304947\n","Predicted:  0.01708595 Actual:  0.008275316263078578\n","Predicted:  0.017066464 Actual:  0.008081281145601804\n","Predicted:  0.017053245 Actual:  0.008341120463933371\n","Predicted:  0.01707627 Actual:  0.008598688712808555\n","Predicted:  0.01705858 Actual:  0.008034339594671286\n","Predicted:  0.017130202 Actual:  0.008658535263377357\n","Predicted:  0.017047644 Actual:  0.007138866169931896\n","Predicted:  0.017046086 Actual:  0.00856461016359511\n","Predicted:  0.017042663 Actual:  0.00788184224153711\n","Predicted:  0.017081 Actual:  0.00821985873029435\n","Predicted:  0.017073061 Actual:  0.007877702193475292\n","Predicted:  0.017007599 Actual:  0.00836744482182375\n","Predicted:  0.017026404 Actual:  0.007327822337091824\n","Predicted:  0.017028019 Actual:  0.0046292722349565996\n","Predicted:  0.017061085 Actual:  0.006543500296789573\n","Predicted:  0.017068876 Actual:  0.008935690917835623\n","Predicted:  0.017096676 Actual:  0.007359335626652641\n","Predicted:  0.016983029 Actual:  0.008552766396029074\n","Predicted:  0.017076494 Actual:  0.008290981039822162\n","Predicted:  0.017059017 Actual:  0.007728856087972663\n","Predicted:  0.017110791 Actual:  0.006603780688186733\n","Predicted:  0.017030202 Actual:  0.004740232125647392\n","Predicted:  0.017056482 Actual:  0.008269943816153522\n","Predicted:  0.01706772 Actual:  0.0074158816953584505\n","Predicted:  0.017036684 Actual:  0.008789513456478423\n","Predicted:  0.017035443 Actual:  0.007480636396552234\n","Predicted:  0.017050635 Actual:  0.00850694390344052\n","Predicted:  0.0169954 Actual:  0.0082104072465808\n","Predicted:  0.017028501 Actual:  0.007751988984369046\n","Predicted:  0.01705202 Actual:  0.008273140562598591\n","Predicted:  0.017078534 Actual:  0.001347799998676541\n","Predicted:  0.017064683 Actual:  0.006557240531990416\n","Predicted:  0.017098114 Actual:  0.008348747155580483\n","Predicted:  0.017034385 Actual:  0.006721562795170041\n","Predicted:  0.017042628 Actual:  0.0089260589764482\n","Predicted:  0.017069 Actual:  0.007382380741913233\n","Predicted:  0.01703745 Actual:  0.008417045539337628\n","Predicted:  0.01705854 Actual:  0.002283896122849058\n","Predicted:  0.017075744 Actual:  0.0077972546000793785\n","Predicted:  0.017098881 Actual:  0.007814794059464308\n","Predicted:  0.017090047 Actual:  0.008884895936730322\n","Predicted:  0.017076187 Actual:  0.008704893225727928\n","Predicted:  0.017067598 Actual:  0.007706219226130476\n","Predicted:  0.017069427 Actual:  0.008458670955087039\n","Predicted:  0.017073808 Actual:  0.008858304548337248\n","Predicted:  0.017061241 Actual:  0.0081749396093386\n","Predicted:  0.017079968 Actual:  0.007295099320669796\n","Predicted:  0.017059062 Actual:  0.008689427235978209\n","Predicted:  0.017053267 Actual:  0.007631516411758535\n","Predicted:  0.017081873 Actual:  0.008275221714072842\n","Predicted:  0.017012723 Actual:  0.008544737075205075\n","Predicted:  0.01706534 Actual:  0.006661926583390029\n","Predicted:  0.01711541 Actual:  0.00599131096267265\n","Predicted:  0.01705712 Actual:  0.008554503027496024\n","Predicted:  0.017071646 Actual:  0.00816941991489993\n","Predicted:  0.017144462 Actual:  0.007523101052894679\n","Predicted:  0.017056132 Actual:  0.008880314756520943\n","Predicted:  0.017070899 Actual:  0.0060571314434648\n","Predicted:  0.017066559 Actual:  0.008873167901974816\n","Predicted:  0.017064828 Actual:  0.007608041190986237\n","Predicted:  0.017054655 Actual:  0.0073248043311878175\n","Predicted:  0.017060826 Actual:  0.007880321256371989\n","Predicted:  0.01706066 Actual:  0.008666933275023325\n","Predicted:  0.017020348 Actual:  0.005947615570223969\n","Predicted:  0.017057609 Actual:  0.007547880913210181\n","Predicted:  0.017032905 Actual:  0.008023546259459238\n","Predicted:  0.017076604 Actual:  0.008899901732651816\n","Predicted:  0.016948216 Actual:  0.008861485204241015\n","Predicted:  0.017105758 Actual:  0.006217091898372107\n","Predicted:  0.017040689 Actual:  0.008517325485840915\n","Predicted:  0.017063435 Actual:  0.008128762785237748\n","Predicted:  0.017071307 Actual:  0.007462281723403297\n","Predicted:  0.017070394 Actual:  0.008422529052697011\n","Predicted:  0.01699557 Actual:  0.007360659032278602\n","Predicted:  0.01705474 Actual:  0.007606072946554257\n","Predicted:  0.017061954 Actual:  0.006799278481364912\n","Predicted:  0.017012361 Actual:  0.008845470768738373\n","Predicted:  0.016973685 Actual:  0.008389908522724268\n","Predicted:  0.017049465 Actual:  0.006015767515095166\n","Predicted:  0.017033534 Actual:  0.00894698815321382\n","Predicted:  0.017044459 Actual:  0.006814975247820737\n","Predicted:  0.017106237 Actual:  0.005335436474790234\n","Predicted:  0.017040025 Actual:  0.008001721023773292\n","Predicted:  0.017027384 Actual:  0.0075394705978749366\n","Predicted:  0.017028268 Actual:  0.008557739765401334\n","Predicted:  0.017104842 Actual:  0.006978128781963258\n","Predicted:  0.017120836 Actual:  0.008502289767903959\n","Predicted:  0.017077764 Actual:  0.007145558085269385\n","Predicted:  0.017045062 Actual:  0.007707085213781717\n","Predicted:  0.01704674 Actual:  0.007238363388043442\n","Predicted:  0.01702765 Actual:  0.008161383800830008\n","Predicted:  0.017025962 Actual:  0.008620440657749417\n","Predicted:  0.01706222 Actual:  0.00866015436444409\n","Predicted:  0.017083555 Actual:  0.00861041127716329\n","Predicted:  0.017139748 Actual:  0.0034483499003395285\n","Predicted:  0.017079197 Actual:  0.005645335321725634\n","Predicted:  0.01708632 Actual:  0.00897523005884418\n","Predicted:  0.01704532 Actual:  0.006482877299563025\n","Predicted:  0.01706104 Actual:  0.00840730567776525\n","Predicted:  0.017097635 Actual:  0.9629351187644231\n","Predicted:  0.017049296 Actual:  0.9982109759829132\n","Predicted:  0.017066255 Actual:  0.9992153976333895\n","Predicted:  0.017041165 Actual:  0.964876485557538\n","MSE:  0.015213162025046457\n"]}],"source":["# Predict\n","y_pred = regressor.predict(X_test)\n","\n","# y_pred_normal = np.reshape(y_pred, (-1,1))\n","# y_test_normal = np.reshape(y_test, (-1,1))\n","\n","# y_pred_normal = scaler.inverse_transform(y_pred_normal)\n","# y_test_normal = scaler.inverse_transform(y_test_normal)\n","\n","y_pred_normal = np.reshape(y_pred, (y_pred.shape[0],))\n","y_test_normal = np.reshape(y_test, (y_test.shape[0],))\n","\n","# Print predicted and actual values\n","for i in range(len(y_pred_normal)):\n","    print(\"Predicted: \", y_pred_normal[i], \"Actual: \", y_test_normal[i])\n","print(\"MSE: \", mean_squared_error(y_test_normal, y_pred_normal))\n","#print(\"Accuracy: \", accuracy_score(y_test, y_pred))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
