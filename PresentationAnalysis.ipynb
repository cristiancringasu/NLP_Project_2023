{"cells":[{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["import json\n","import pprint\n","\n","import datetime\n","import requests\n","import gzip\n","import subprocess\n","import re\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import yfinance as yf\n","from finta import TA\n","\n","from sklearn import svm\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier \n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, accuracy_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout, Softmax\n","\n","import nltk\n","from nltk.parse.corenlp import CoreNLPServer, CoreNLPParser, CoreNLPDependencyParser\n","from nltk.tokenize import sent_tokenize\n","from collections import Counter\n","\n","import stanfordnlp\n","# import ngram for ngram analysis\n","from nltk.util import ngrams\n","\n","import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# %pip install --force-reinstall -v \"ipywidgets == 7.7.2\"\n","# %pip install --force-reinstall -v \"jupyterlab_widgets == 1.1.1\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /Users/cristal/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /Users/cristal/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","\n","java_path = \"/Library/Java/JavaVirtualMachines/jdk-19.jdk/Contents/Home/bin/java\"\n","os.environ['JAVAHOME'] = java_path\n","nltk.internals.config_java(java_path)\n","\n","CORENLP_JAR = os.path.join(\"models\", \"stanford-corenlp-4.5.4\", \"stanford-corenlp-4.5.4.jar\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["STANFORD = os.path.join(\"models\", \"stanford-corenlp-4.5.4\")\n","\n","# # Create the server\n","# server = CoreNLPServer(\n","#     os.path.join(STANFORD, \"stanford-corenlp-4.5.4.jar\"),\n","#     os.path.join(STANFORD, \"stanford-corenlp-4.5.4-models.jar\"),\n","# )\n","\n","# # Start the server in the background\n","# server.start()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus size:  1224\n"]}],"source":["corpus_path = \"data/formatted_corpus.json\"\n","with open(corpus_path, 'r', encoding='utf-8') as corpus_json:\n","    corpus = json.load(corpus_json)\n","print(\"Corpus size: \", len(corpus))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Visualize data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["FKRE ratings: {'Fairly easy': 566, 'Plain English': 368, 'Easy': 178, 'Fairly difficult': 94, 'Very easy': 8, 'Difficult': 10}\n","FKRE scores: min 41.7, max 94.9, average 71.62647058823535\n","Words per minute: min 71.0, max 275.0, average 160.2393790849673\n","Words in the New Academic Word List: min 1, max 61, average 19.40767973856209\n","Words in the New General Service List: min 224, max 645, average 415.577614379085\n","Length in seconds: min 481, max 1199, average 884.9648692810457\n","Number of views: min 231850, max 75063761, average 2817607.3333333335\n","Number of likes: min 10000, max 22000000, average 175500.0\n","Number of likes per view: min 0.027548623320160084, max 0.2995794216171842, average 0.03349684151685497\n"]},{"data":{"text/plain":["<Figure size 5000x2000 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["FKRE_ratings = {}\n","FKRE_scores = []\n","WPMs = []\n","NAWLs = []\n","NGSLs = []\n","seconds = []\n","view_counts = []\n","like_counts = []\n","likes_per_view = []\n","for talk in corpus:\n","    if talk['FKRE_rating'] in FKRE_ratings:\n","        FKRE_ratings[talk['FKRE_rating']] += 1\n","    else:\n","        FKRE_ratings[talk['FKRE_rating']] = 1\n","    FKRE_scores.append(talk['FKRE_score'])\n","    WPMs.append(talk['WPM'])\n","    NAWLs.append(talk['NAWL'])\n","    NGSLs.append(talk['NGSL'])\n","    seconds.append(talk['seconds'])\n","    view_counts.append(talk['view_count'])\n","    like_counts.append(talk['like_count'])\n","    likes_per_view.append(talk['likes_per_view'])\n","\n","print(f'FKRE ratings: {FKRE_ratings}')\n","print(f'FKRE scores: min {min(FKRE_scores)}, max {max(FKRE_scores)}, average {sum(FKRE_scores) / len(FKRE_scores)}')\n","print(f'Words per minute: min {min(WPMs)}, max {max(WPMs)}, average {sum(WPMs) / len(WPMs)}')\n","print(f'Words in the New Academic Word List: min {min(NAWLs)}, max {max(NAWLs)}, average {sum(NAWLs) / len(NAWLs)}')\n","print(f'Words in the New General Service List: min {min(NGSLs)}, max {max(NGSLs)}, average {sum(NGSLs) / len(NGSLs)}')\n","print(f'Length in seconds: min {min(seconds)}, max {max(seconds)}, average {sum(seconds) / len(seconds)}')\n","print(f'Number of views: min {min(view_counts)}, max {max(view_counts)}, average {sum(view_counts) / len(view_counts)}')\n","print(f'Number of likes: min {min(like_counts)}, max {max(like_counts)}, average {sum(like_counts) / len(like_counts)}')\n","print(f'Number of likes per view: min {min(likes_per_view)}, max {max(likes_per_view)}, average {sum(likes_per_view) / len(likes_per_view)}')\n","\n","plt.figure(figsize=(50, 20))\n","\n","plt.bar(range(len(FKRE_scores)), FKRE_scores)\n","plt.title('FKRE scores')\n","plt.savefig(f'data/stats/FKRE_scores.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(WPMs)), WPMs)\n","plt.title('Words per minute')\n","plt.savefig(f'data/stats/WPMs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(NAWLs)), NAWLs)\n","plt.title('Words in the New Academic Word List')\n","plt.savefig(f'data/stats/NAWLs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(NGSLs)), NGSLs)\n","plt.title('Words in the New General Service List')\n","plt.savefig(f'data/stats/NGSLs.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(seconds)), seconds)\n","plt.title('Length in seconds')\n","plt.savefig(f'data/stats/seconds.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(view_counts)), view_counts)\n","plt.title('Number of views')\n","plt.savefig(f'data/stats/view_counts.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(like_counts)), like_counts)\n","plt.title('Number of likes')\n","plt.savefig(f'data/stats/like_counts.jpg')\n","plt.clf()\n","\n","plt.bar(range(len(likes_per_view)), likes_per_view)\n","plt.title('Number of likes per view')\n","plt.savefig(f'data/stats/likes_per_view.jpg')\n","plt.clf()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Normalize metrics"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n"]}],"source":["weights = np.array([4, 1, 1, 1, 1, 5, 3, 2, 1])\n","normalized_array = np.divide(weights, np.sum(weights))\n","print(sum(normalized_array))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Cluster 2: Rank 1, 22 talks\n","Cluster 1: Rank 2, 171 talks\n","Cluster 3: Rank 3, 386 talks\n","Cluster 4: Rank 4, 411 talks\n","Cluster 0: Rank 5, 234 talks\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>FKRE_rating</th>\n","      <th>FKRE_score</th>\n","      <th>WPM</th>\n","      <th>NAWL</th>\n","      <th>NGSL</th>\n","      <th>URL</th>\n","      <th>seconds</th>\n","      <th>view_count</th>\n","      <th>like_count</th>\n","      <th>transcript</th>\n","      <th>raw_transcript</th>\n","      <th>likes_per_view</th>\n","      <th>total_responses</th>\n","      <th>laughter</th>\n","      <th>applause</th>\n","      <th>cheering</th>\n","      <th>popularity_score</th>\n","      <th>popularity_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Aaron Huey: America's native prisoners of war</td>\n","      <td>Fairly easy</td>\n","      <td>70.0</td>\n","      <td>146.0</td>\n","      <td>12</td>\n","      <td>458</td>\n","      <td>https://www.ted.com/talks/aaron_huey</td>\n","      <td>916</td>\n","      <td>1970692</td>\n","      <td>59000</td>\n","      <td>[{'sentence': 'I'm here today to show my photo...</td>\n","      <td>i'm here today to show my photographs of the l...</td>\n","      <td>0.029939</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.211961</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abha Dawesar: Life in the \"digital now\"</td>\n","      <td>Fairly easy</td>\n","      <td>74.7</td>\n","      <td>169.0</td>\n","      <td>16</td>\n","      <td>435</td>\n","      <td>https://www.ted.com/talks/abha_dawesar_life_in...</td>\n","      <td>713</td>\n","      <td>1369143</td>\n","      <td>41000</td>\n","      <td>[{'sentence': 'I was in New York during Hurric...</td>\n","      <td>i was in new york during hurricane sandy, and ...</td>\n","      <td>0.029946</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.222254</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Abraham Verghese: A doctor's touch</td>\n","      <td>Fairly easy</td>\n","      <td>70.1</td>\n","      <td>170.0</td>\n","      <td>41</td>\n","      <td>526</td>\n","      <td>https://www.ted.com/talks/abraham_verghese_a_d...</td>\n","      <td>1100</td>\n","      <td>1992577</td>\n","      <td>59000</td>\n","      <td>[{'sentence': 'A few months ago, a 40 year-old...</td>\n","      <td>a few months ago, a 40 year-old woman came to ...</td>\n","      <td>0.029610</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.265658</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Adam Davidson: What we learned from teetering ...</td>\n","      <td>Plain English</td>\n","      <td>61.1</td>\n","      <td>165.0</td>\n","      <td>24</td>\n","      <td>546</td>\n","      <td>https://www.ted.com/talks/adam_davidson_what_w...</td>\n","      <td>1177</td>\n","      <td>838052</td>\n","      <td>25000</td>\n","      <td>[{'sentence': 'So a friend of mine who's a pol...</td>\n","      <td>so a friend of mine who's a political scientis...</td>\n","      <td>0.029831</td>\n","      <td>{'laughter': 0, 'applause': 1, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.222199</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Adam Garone: Healthier men, one moustache at a...</td>\n","      <td>Fairly easy</td>\n","      <td>74.2</td>\n","      <td>171.0</td>\n","      <td>12</td>\n","      <td>416</td>\n","      <td>https://www.ted.com/talks/adam_garone_healthie...</td>\n","      <td>989</td>\n","      <td>755891</td>\n","      <td>22000</td>\n","      <td>[{'sentence': 'I think the beautiful Malin [Ak...</td>\n","      <td>i think the beautiful malin [akerman] put it p...</td>\n","      <td>0.029105</td>\n","      <td>{'laughter': 28, 'applause': 4, 'cheering': 0}</td>\n","      <td>28</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.318285</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1219</th>\n","      <td>Zahra' Langhi: Why Libya's revolution didn't w...</td>\n","      <td>Difficult</td>\n","      <td>48.2</td>\n","      <td>104.0</td>\n","      <td>18</td>\n","      <td>255</td>\n","      <td>https://www.ted.com/talks/zahra_langhi_why_lib...</td>\n","      <td>576</td>\n","      <td>561666</td>\n","      <td>16000</td>\n","      <td>[{'sentence': 'I have never, ever forgotten th...</td>\n","      <td>i have never, ever forgotten the words of my g...</td>\n","      <td>0.028487</td>\n","      <td>{'laughter': 0, 'applause': 4, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.090970</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1220</th>\n","      <td>Zainab Salbi: Women, wartime and the dream of ...</td>\n","      <td>Fairly easy</td>\n","      <td>74.1</td>\n","      <td>128.0</td>\n","      <td>14</td>\n","      <td>413</td>\n","      <td>https://www.ted.com/talks/zainab_salbi</td>\n","      <td>1054</td>\n","      <td>618890</td>\n","      <td>18000</td>\n","      <td>[{'sentence': 'I woke up in the middle of the ...</td>\n","      <td>i woke up in the middle of the night with the ...</td>\n","      <td>0.029084</td>\n","      <td>{'laughter': 1, 'applause': 1, 'cheering': 0}</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.231153</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1221</th>\n","      <td>Zak Ebrahim: I am the son of a terrorist. Here...</td>\n","      <td>Plain English</td>\n","      <td>67.6</td>\n","      <td>152.0</td>\n","      <td>11</td>\n","      <td>368</td>\n","      <td>https://www.ted.com/talks/zak_ebrahim_i_am_the...</td>\n","      <td>545</td>\n","      <td>6602165</td>\n","      <td>198000</td>\n","      <td>[{'sentence': 'On November 5th, 1990, a man na...</td>\n","      <td>on november 5th, 1990, a man named el-sayyid n...</td>\n","      <td>0.029990</td>\n","      <td>{'laughter': 0, 'applause': 4, 'cheering': 0}</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.187294</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1222</th>\n","      <td>Zeresenay Alemseged: The search for humanity's...</td>\n","      <td>Fairly easy</td>\n","      <td>71.9</td>\n","      <td>160.0</td>\n","      <td>19</td>\n","      <td>456</td>\n","      <td>https://www.ted.com/talks/zeresenay_alemseged_...</td>\n","      <td>943</td>\n","      <td>1228159</td>\n","      <td>36000</td>\n","      <td>[{'sentence': 'I have 18 minutes to tell you w...</td>\n","      <td>i have 18 minutes to tell you what happened ov...</td>\n","      <td>0.029312</td>\n","      <td>{'laughter': 4, 'applause': 1, 'cheering': 0}</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.239127</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1223</th>\n","      <td>Zeynep Tufekci: Online social change: easy to ...</td>\n","      <td>Fairly difficult</td>\n","      <td>58.7</td>\n","      <td>143.0</td>\n","      <td>18</td>\n","      <td>466</td>\n","      <td>https://www.ted.com/talks/zeynep_tufekci_how_t...</td>\n","      <td>967</td>\n","      <td>1407711</td>\n","      <td>42000</td>\n","      <td>[{'sentence': 'So recently, we heard a lot abo...</td>\n","      <td>so recently, we heard a lot about how social m...</td>\n","      <td>0.029836</td>\n","      <td>{'laughter': 1, 'applause': 1, 'cheering': 0}</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.178565</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1224 rows × 19 columns</p>\n","</div>"],"text/plain":["                                                  title       FKRE_rating  \\\n","0         Aaron Huey: America's native prisoners of war       Fairly easy   \n","1               Abha Dawesar: Life in the \"digital now\"       Fairly easy   \n","2                    Abraham Verghese: A doctor's touch       Fairly easy   \n","3     Adam Davidson: What we learned from teetering ...     Plain English   \n","4     Adam Garone: Healthier men, one moustache at a...       Fairly easy   \n","...                                                 ...               ...   \n","1219  Zahra' Langhi: Why Libya's revolution didn't w...         Difficult   \n","1220  Zainab Salbi: Women, wartime and the dream of ...       Fairly easy   \n","1221  Zak Ebrahim: I am the son of a terrorist. Here...     Plain English   \n","1222  Zeresenay Alemseged: The search for humanity's...       Fairly easy   \n","1223  Zeynep Tufekci: Online social change: easy to ...  Fairly difficult   \n","\n","      FKRE_score    WPM  NAWL  NGSL  \\\n","0           70.0  146.0    12   458   \n","1           74.7  169.0    16   435   \n","2           70.1  170.0    41   526   \n","3           61.1  165.0    24   546   \n","4           74.2  171.0    12   416   \n","...          ...    ...   ...   ...   \n","1219        48.2  104.0    18   255   \n","1220        74.1  128.0    14   413   \n","1221        67.6  152.0    11   368   \n","1222        71.9  160.0    19   456   \n","1223        58.7  143.0    18   466   \n","\n","                                                    URL  seconds  view_count  \\\n","0                  https://www.ted.com/talks/aaron_huey      916     1970692   \n","1     https://www.ted.com/talks/abha_dawesar_life_in...      713     1369143   \n","2     https://www.ted.com/talks/abraham_verghese_a_d...     1100     1992577   \n","3     https://www.ted.com/talks/adam_davidson_what_w...     1177      838052   \n","4     https://www.ted.com/talks/adam_garone_healthie...      989      755891   \n","...                                                 ...      ...         ...   \n","1219  https://www.ted.com/talks/zahra_langhi_why_lib...      576      561666   \n","1220             https://www.ted.com/talks/zainab_salbi     1054      618890   \n","1221  https://www.ted.com/talks/zak_ebrahim_i_am_the...      545     6602165   \n","1222  https://www.ted.com/talks/zeresenay_alemseged_...      943     1228159   \n","1223  https://www.ted.com/talks/zeynep_tufekci_how_t...      967     1407711   \n","\n","      like_count                                         transcript  \\\n","0          59000  [{'sentence': 'I'm here today to show my photo...   \n","1          41000  [{'sentence': 'I was in New York during Hurric...   \n","2          59000  [{'sentence': 'A few months ago, a 40 year-old...   \n","3          25000  [{'sentence': 'So a friend of mine who's a pol...   \n","4          22000  [{'sentence': 'I think the beautiful Malin [Ak...   \n","...          ...                                                ...   \n","1219       16000  [{'sentence': 'I have never, ever forgotten th...   \n","1220       18000  [{'sentence': 'I woke up in the middle of the ...   \n","1221      198000  [{'sentence': 'On November 5th, 1990, a man na...   \n","1222       36000  [{'sentence': 'I have 18 minutes to tell you w...   \n","1223       42000  [{'sentence': 'So recently, we heard a lot abo...   \n","\n","                                         raw_transcript  likes_per_view  \\\n","0     i'm here today to show my photographs of the l...        0.029939   \n","1     i was in new york during hurricane sandy, and ...        0.029946   \n","2     a few months ago, a 40 year-old woman came to ...        0.029610   \n","3     so a friend of mine who's a political scientis...        0.029831   \n","4     i think the beautiful malin [akerman] put it p...        0.029105   \n","...                                                 ...             ...   \n","1219  i have never, ever forgotten the words of my g...        0.028487   \n","1220  i woke up in the middle of the night with the ...        0.029084   \n","1221  on november 5th, 1990, a man named el-sayyid n...        0.029990   \n","1222  i have 18 minutes to tell you what happened ov...        0.029312   \n","1223  so recently, we heard a lot about how social m...        0.029836   \n","\n","                                     total_responses  laughter  applause  \\\n","0      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","1      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","2      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","3      {'laughter': 0, 'applause': 1, 'cheering': 0}         0         1   \n","4     {'laughter': 28, 'applause': 4, 'cheering': 0}        28         4   \n","...                                              ...       ...       ...   \n","1219   {'laughter': 0, 'applause': 4, 'cheering': 0}         0         4   \n","1220   {'laughter': 1, 'applause': 1, 'cheering': 0}         1         1   \n","1221   {'laughter': 0, 'applause': 4, 'cheering': 0}         0         4   \n","1222   {'laughter': 4, 'applause': 1, 'cheering': 0}         4         1   \n","1223   {'laughter': 1, 'applause': 1, 'cheering': 0}         1         1   \n","\n","      cheering  popularity_score  popularity_category  \n","0            0          0.211961                    4  \n","1            0          0.222254                    4  \n","2            0          0.265658                    3  \n","3            0          0.222199                    4  \n","4            0          0.318285                    1  \n","...        ...               ...                  ...  \n","1219         0          0.090970                    0  \n","1220         0          0.231153                    4  \n","1221         0          0.187294                    0  \n","1222         0          0.239127                    4  \n","1223         0          0.178565                    0  \n","\n","[1224 rows x 19 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["data = pd.DataFrame(corpus)\n","\n","# Extract laughter, applause, and cheering counts from the \"total_responses\" dictionary\n","data['laughter'] = data['total_responses'].apply(lambda x: x['laughter'])\n","data['applause'] = data['total_responses'].apply(lambda x: x['applause'])\n","data['cheering'] = data['total_responses'].apply(lambda x: x['cheering'])\n","\n","# Normalize the metrics\n","scaler = MinMaxScaler()\n","normalized_data = scaler.fit_transform(data[['FKRE_score', 'WPM', 'NAWL', 'NGSL', 'seconds', 'likes_per_view', 'laughter', 'applause', 'cheering']])\n","weights = np.array([4, 1, 1, 1, 1, 5, 3, 2, 1])\n","weights = np.divide(weights, np.sum(weights))\n","\n","# Calculate the popularity score\n","popularity_score = np.dot(normalized_data, weights)\n","\n","# Add the popularity score to the DataFrame\n","data['popularity_score'] = popularity_score\n","\n","# Classify the talks using K-means clustering\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","data['popularity_category'] = kmeans.fit_predict(data[['popularity_score']])\n","\n","# Calculate the average popularity score for each cluster\n","cluster_popularity = data.groupby('popularity_category')['popularity_score'].mean()\n","\n","# Sort the clusters by popularity score in descending order\n","sorted_clusters = cluster_popularity.sort_values(ascending=False)\n","\n","# Count the number of talks in each cluster\n","talks_per_cluster = data['popularity_category'].value_counts()\n","\n","# Print the ranking of the clusters by popularity score\n","for i, cluster in enumerate(sorted_clusters.index):\n","    print(f\"Cluster {cluster}: Rank {i+1}, {talks_per_cluster[cluster]} talks\")\n","\n","display(data)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#ciorna\n","#TODO: get it to work\n","\n","# output = subprocess.check_output(['java', '-cp', CORENLP_JAR, 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize', '-outputFormat', 'json'], input='This is a test sentence.'.encode())\n","# print(output)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use NLP techniques to extract extra features (WIP)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Annotate: {'sentences': [{'index': 0, 'parse': '(ROOT\\n  (NP\\n    (NP (DT A) (NN blog) (NN post))\\n    (VP (VBG using)\\n      (NP (NNP Stanford) (NN CoreNLP) (NN Server)))\\n    (. .)))', 'basicDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'post'}, {'dep': 'det', 'governor': 3, 'governorGloss': 'post', 'dependent': 1, 'dependentGloss': 'A'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'post', 'dependent': 2, 'dependentGloss': 'blog'}, {'dep': 'acl', 'governor': 3, 'governorGloss': 'post', 'dependent': 4, 'dependentGloss': 'using'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 5, 'dependentGloss': 'Stanford'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 6, 'dependentGloss': 'CoreNLP'}, {'dep': 'obj', 'governor': 4, 'governorGloss': 'using', 'dependent': 7, 'dependentGloss': 'Server'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'post', 'dependent': 8, 'dependentGloss': '.'}], 'enhancedDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'post'}, {'dep': 'det', 'governor': 3, 'governorGloss': 'post', 'dependent': 1, 'dependentGloss': 'A'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'post', 'dependent': 2, 'dependentGloss': 'blog'}, {'dep': 'acl', 'governor': 3, 'governorGloss': 'post', 'dependent': 4, 'dependentGloss': 'using'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 5, 'dependentGloss': 'Stanford'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 6, 'dependentGloss': 'CoreNLP'}, {'dep': 'obj', 'governor': 4, 'governorGloss': 'using', 'dependent': 7, 'dependentGloss': 'Server'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'post', 'dependent': 8, 'dependentGloss': '.'}], 'enhancedPlusPlusDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'post'}, {'dep': 'det', 'governor': 3, 'governorGloss': 'post', 'dependent': 1, 'dependentGloss': 'A'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'post', 'dependent': 2, 'dependentGloss': 'blog'}, {'dep': 'acl', 'governor': 3, 'governorGloss': 'post', 'dependent': 4, 'dependentGloss': 'using'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 5, 'dependentGloss': 'Stanford'}, {'dep': 'compound', 'governor': 7, 'governorGloss': 'Server', 'dependent': 6, 'dependentGloss': 'CoreNLP'}, {'dep': 'obj', 'governor': 4, 'governorGloss': 'using', 'dependent': 7, 'dependentGloss': 'Server'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'post', 'dependent': 8, 'dependentGloss': '.'}], 'entitymentions': [], 'tokens': [{'index': 1, 'word': 'A', 'originalText': 'A', 'lemma': 'a', 'characterOffsetBegin': 0, 'characterOffsetEnd': 1, 'pos': 'DT', 'ner': 'O', 'speaker': 'PER0', 'before': '', 'after': ' '}, {'index': 2, 'word': 'blog', 'originalText': 'blog', 'lemma': 'blog', 'characterOffsetBegin': 2, 'characterOffsetEnd': 6, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'post', 'originalText': 'post', 'lemma': 'post', 'characterOffsetBegin': 7, 'characterOffsetEnd': 11, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'using', 'originalText': 'using', 'lemma': 'use', 'characterOffsetBegin': 12, 'characterOffsetEnd': 17, 'pos': 'VBG', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'Stanford', 'originalText': 'Stanford', 'lemma': 'Stanford', 'characterOffsetBegin': 18, 'characterOffsetEnd': 26, 'pos': 'NNP', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 6, 'word': 'CoreNLP', 'originalText': 'CoreNLP', 'lemma': 'corenlp', 'characterOffsetBegin': 27, 'characterOffsetEnd': 34, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 7, 'word': 'Server', 'originalText': 'Server', 'lemma': 'server', 'characterOffsetBegin': 35, 'characterOffsetEnd': 41, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ''}, {'index': 8, 'word': '.', 'originalText': '.', 'lemma': '.', 'characterOffsetBegin': 41, 'characterOffsetEnd': 42, 'pos': '.', 'ner': 'O', 'speaker': 'PER0', 'before': '', 'after': ' '}]}, {'index': 1, 'parse': '(ROOT\\n  (S\\n    (VP (VB Visit)\\n      (NP (ADD www.khalidalnajjar.com))\\n      (PP (IN for)\\n        (NP (JJR more) (NNS details))))\\n    (. .)))', 'basicDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 1, 'dependentGloss': 'Visit'}, {'dep': 'obj', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 2, 'dependentGloss': 'www.khalidalnajjar.com'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'details', 'dependent': 3, 'dependentGloss': 'for'}, {'dep': 'amod', 'governor': 5, 'governorGloss': 'details', 'dependent': 4, 'dependentGloss': 'more'}, {'dep': 'obl', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 5, 'dependentGloss': 'details'}, {'dep': 'punct', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 6, 'dependentGloss': '.'}], 'enhancedDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 1, 'dependentGloss': 'Visit'}, {'dep': 'obj', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 2, 'dependentGloss': 'www.khalidalnajjar.com'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'details', 'dependent': 3, 'dependentGloss': 'for'}, {'dep': 'amod', 'governor': 5, 'governorGloss': 'details', 'dependent': 4, 'dependentGloss': 'more'}, {'dep': 'obl:for', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 5, 'dependentGloss': 'details'}, {'dep': 'punct', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 6, 'dependentGloss': '.'}], 'enhancedPlusPlusDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 1, 'dependentGloss': 'Visit'}, {'dep': 'obj', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 2, 'dependentGloss': 'www.khalidalnajjar.com'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'details', 'dependent': 3, 'dependentGloss': 'for'}, {'dep': 'amod', 'governor': 5, 'governorGloss': 'details', 'dependent': 4, 'dependentGloss': 'more'}, {'dep': 'obl:for', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 5, 'dependentGloss': 'details'}, {'dep': 'punct', 'governor': 1, 'governorGloss': 'Visit', 'dependent': 6, 'dependentGloss': '.'}], 'entitymentions': [{'docTokenBegin': 9, 'docTokenEnd': 10, 'tokenBegin': 1, 'tokenEnd': 2, 'text': 'www.khalidalnajjar.com', 'characterOffsetBegin': 49, 'characterOffsetEnd': 71, 'ner': 'URL'}], 'tokens': [{'index': 1, 'word': 'Visit', 'originalText': 'Visit', 'lemma': 'visit', 'characterOffsetBegin': 43, 'characterOffsetEnd': 48, 'pos': 'VB', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 2, 'word': 'www.khalidalnajjar.com', 'originalText': 'www.khalidalnajjar.com', 'lemma': 'www.khalidalnajjar.com', 'characterOffsetBegin': 49, 'characterOffsetEnd': 71, 'pos': 'ADD', 'ner': 'URL', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'for', 'originalText': 'for', 'lemma': 'for', 'characterOffsetBegin': 72, 'characterOffsetEnd': 75, 'pos': 'IN', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'more', 'originalText': 'more', 'lemma': 'more', 'characterOffsetBegin': 76, 'characterOffsetEnd': 80, 'pos': 'JJR', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'details', 'originalText': 'details', 'lemma': 'detail', 'characterOffsetBegin': 81, 'characterOffsetEnd': 88, 'pos': 'NNS', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ''}, {'index': 6, 'word': '.', 'originalText': '.', 'lemma': '.', 'characterOffsetBegin': 88, 'characterOffsetEnd': 89, 'pos': '.', 'ner': 'O', 'speaker': 'PER0', 'before': '', 'after': ''}]}], 'corefs': {'1': [{'id': 1, 'text': 'A blog post using Stanford CoreNLP Server .', 'type': 'NOMINAL', 'number': 'SINGULAR', 'gender': 'UNKNOWN', 'animacy': 'INANIMATE', 'startIndex': 1, 'endIndex': 9, 'headIndex': 3, 'sentNum': 1, 'position': [1, 1], 'isRepresentativeMention': True}], '2': [{'id': 2, 'text': 'Stanford CoreNLP Server', 'type': 'NOMINAL', 'number': 'SINGULAR', 'gender': 'NEUTRAL', 'animacy': 'INANIMATE', 'startIndex': 5, 'endIndex': 8, 'headIndex': 7, 'sentNum': 1, 'position': [1, 2], 'isRepresentativeMention': True}], '3': [{'id': 3, 'text': 'www.khalidalnajjar.com', 'type': 'NOMINAL', 'number': 'UNKNOWN', 'gender': 'UNKNOWN', 'animacy': 'UNKNOWN', 'startIndex': 2, 'endIndex': 3, 'headIndex': 2, 'sentNum': 2, 'position': [2, 1], 'isRepresentativeMention': True}], '4': [{'id': 4, 'text': 'more details', 'type': 'NOMINAL', 'number': 'PLURAL', 'gender': 'UNKNOWN', 'animacy': 'INANIMATE', 'startIndex': 4, 'endIndex': 6, 'headIndex': 5, 'sentNum': 2, 'position': [2, 2], 'isRepresentativeMention': True}]}}\n","POS: [('A', 'DT'), ('blog', 'NN'), ('post', 'NN'), ('using', 'VBG'), ('Stanford', 'NNP'), ('CoreNLP', 'NN'), ('Server', 'NN'), ('.', '.'), ('Visit', 'VB'), ('www.khalidalnajjar.com', 'ADD'), ('for', 'IN'), ('more', 'JJR'), ('details', 'NNS'), ('.', '.')]\n","Tokens: ['A', 'blog', 'post', 'using', 'Stanford', 'CoreNLP', 'Server', '.', 'Visit', 'www.khalidalnajjar.com', 'for', 'more', 'details', '.']\n","NER: [('A', 'O'), ('blog', 'O'), ('post', 'O'), ('using', 'O'), ('Stanford', 'O'), ('CoreNLP', 'O'), ('Server', 'O'), ('.', 'O'), ('Visit', 'O'), ('www.khalidalnajjar.com', 'URL'), ('for', 'O'), ('more', 'O'), ('details', 'O'), ('.', 'O')]\n","Parse: (ROOT\n","  (NP\n","    (NP (DT A) (NN blog) (NN post))\n","    (VP (VBG using)\n","      (NP (NNP Stanford) (NN CoreNLP) (NN Server)))\n","    (. .)))\n","Dep Parse: [('ROOT', 0, 3), ('det', 3, 1), ('compound', 3, 2), ('acl', 3, 4), ('compound', 7, 5), ('compound', 7, 6), ('obj', 4, 7), ('punct', 3, 8), ('ROOT', 0, 1), ('obj', 1, 2), ('case', 5, 3), ('amod', 5, 4), ('obl', 1, 5), ('punct', 1, 6)]\n"]}],"source":["from stanfordcorenlp import StanfordCoreNLP\n","import logging\n","import json\n","\n","class StanfordNLP:\n","    def __init__(self, host='http://localhost', port=8000):\n","        self.nlp = StanfordCoreNLP(host, port=port,\n","        timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n","        self.props = {\n","        'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n","        'pipelineLanguage': 'en',\n","        'outputFormat': 'json'\n","                }\n","    def word_tokenize(self, sentence):\n","        return self.nlp.word_tokenize(sentence)\n","    def pos(self, sentence):\n","        return self.nlp.pos_tag(sentence)\n","    def ner(self, sentence):\n","        return self.nlp.ner(sentence)\n","    def parse(self, sentence):\n","        return self.nlp.parse(sentence)\n","    def dependency_parse(self, sentence):\n","        return self.nlp.dependency_parse(sentence)\n","    def annotate(self, sentence):\n","        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n","    @staticmethod\n","    def tokens_to_dict(_tokens):\n","        tokens = defaultdict(dict)\n","        for token in _tokens:\n","            tokens[int(token['index'])] = {\n","            'word': token['word'],\n","            'lemma': token['lemma'],\n","            'pos': token['pos'],\n","            'ner': token['ner']\n","                        }\n","        return tokens\n","\n","sNLP = StanfordNLP()\n","text = 'A blog post using Stanford CoreNLP Server. Visit www.khalidalnajjar.com for more details.'\n","print (\"Annotate:\", sNLP.annotate(text))\n","print (\"POS:\", sNLP.pos(text))\n","print (\"Tokens:\", sNLP.word_tokenize(text))\n","print (\"NER:\", sNLP.ner(text))\n","print (\"Parse:\", sNLP.parse(text))\n","print (\"Dep Parse:\", sNLP.dependency_parse(text))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing rows:   0%|          | 0/1224 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Processing row 0\n"]},{"name":"stderr","output_type":"stream","text":["Processing window: 100%|██████████| 131/131 [01:15<00:00,  1.75it/s]\n","Processing rows:   0%|          | 1/1224 [01:15<25:29:49, 75.05s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing row 1\n"]},{"name":"stderr","output_type":"stream","text":["Processing window:  41%|████      | 43/106 [00:38<00:56,  1.12it/s]\n","Processing rows:   0%|          | 1/1224 [01:53<38:30:37, 113.36s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m window_sentences \u001b[39m=\u001b[39m sentences[start:end]\n\u001b[1;32m     78\u001b[0m \u001b[39m# Compute the features for the current window\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m features \u001b[39m=\u001b[39m compute_features(window_sentences)\n\u001b[1;32m     80\u001b[0m features_list\u001b[39m.\u001b[39mappend(features)\n\u001b[1;32m     82\u001b[0m \u001b[39m# Move the window one sentence forward\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mcompute_features\u001b[0;34m(window_sentences)\u001b[0m\n\u001b[1;32m      8\u001b[0m num_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(window_sentences)\n\u001b[1;32m      9\u001b[0m \u001b[39m# print(window_text)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Compute the mean length of clause\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# output = subprocess.check_output(['java', '-cp', CORENLP_JAR, 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit,pos,parse', '-outputFormat', 'json'], input=window_text.encode())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# output = output.decode()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m output \u001b[39m=\u001b[39m sNLP\u001b[39m.\u001b[39;49mannotate(window_text)\n\u001b[1;32m     14\u001b[0m parse_trees \u001b[39m=\u001b[39m [sentence[\u001b[39m'\u001b[39m\u001b[39mparse\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m output[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     15\u001b[0m clauses \u001b[39m=\u001b[39m []\n","Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mStanfordNLP.annotate\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mannotate\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlp\u001b[39m.\u001b[39;49mannotate(sentence, properties\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprops))\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/stanfordcorenlp/corenlp.py:153\u001b[0m, in \u001b[0;36mStanfordCoreNLP.annotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info\u001b[39m.\u001b[39mmajor \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    151\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mproperties\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m(properties)}, data\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m    154\u001b[0m                   headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mConnection\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mclose\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mtext\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:445\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    441\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n","File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:440\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    441\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def compute_features(window_sentences):\n","    \"\"\"Compute the features for a given window\"\"\"\n","\n","    # Combine all sentences in the window\n","    window_text = ' '.join(window_sentences)\n","    \n","    # Compute the number of sentences in the window\n","    num_sentences = len(window_sentences)\n","    # print(window_text)\n","    # Compute the mean length of clause\n","    # output = subprocess.check_output(['java', '-cp', CORENLP_JAR, 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit,pos,parse', '-outputFormat', 'json'], input=window_text.encode())\n","    # output = output.decode()\n","    output = sNLP.annotate(window_text)\n","    parse_trees = [sentence['parse'] for sentence in output['sentences']]\n","    clauses = []\n","    for parse_tree in parse_trees:\n","        clauses.extend(re.findall(r'\\(S\\n.*?\\)', parse_tree))\n","    clause_lengths = [len(re.findall(r'\\(VB.*?\\)', clause)) + 1 for clause in clauses]\n","    mean_clause_length = sum(clause_lengths) / len(clause_lengths)\n","    \n","    # Compute clauses per sentence\n","    clauses_per_sentence = len(clauses) / num_sentences\n","    \n","    # Compute coordinate phrases per clause\n","    coordinate_phrases = []\n","    for parse_tree in parse_trees:\n","        coordinate_phrases.extend(re.findall(r'\\(CC .*?\\)', parse_tree))\n","    coordinate_phrases_per_clause = len(coordinate_phrases) / len(clauses)\n","    \n","    # Compute complex nominals per clause\n","    complex_nominals = []\n","    for parse_tree in parse_trees:\n","        complex_nominals.extend(re.findall(r'\\(NP .*?SBAR', parse_tree))\n","    complex_nominals_per_clause = len(complex_nominals) / len(clauses)\n","    \n","    # Compute type-token ratio\n","    words = window_text.split()\n","    type_token_ratio = len(set(words)) / len(words)\n","    \n","    # Compute n-grams frequency features\n","    n_grams = [2, 3, 4, 5]\n","    n_grams_freq = {}\n","    for n in n_grams:\n","        n_grams_freq[f\"{n}-\"] = Counter(ngrams(words, n))\n","    \n","    return {\n","        'mean_clause_length': mean_clause_length,\n","        'clauses_per_sentence': clauses_per_sentence,\n","        'coordinate_phrases_per_clause': coordinate_phrases_per_clause,\n","        'complex_nominals_per_clause': complex_nominals_per_clause,\n","        'type_token_ratio': type_token_ratio,\n","        'n_grams_freq': n_grams_freq\n","    }\n","\n","# Define the sliding window size\n","window_size = 5\n","window_features = []\n","\n","# Iterate through the raw_transcript column\n","for index, row in tqdm.tqdm(data.iterrows(), total=data.shape[0], desc='Processing rows', position=0):\n","    print(\"Processing row\", index)\n","    \n","    transcript = row['raw_transcript']\n","    \n","    # Tokenize the transcript into sentences\n","    sentences = sent_tokenize(transcript)\n","\n","    # Initialize the window start and end indices\n","    start = 0\n","    end = window_size\n","\n","    # Iterate through the sentences using the sliding window\n","    features_list = []\n","    for end in tqdm.tqdm(range(window_size, len(sentences) + 1), desc='Processing window', position=0):\n","        # Extract the sentences in the current window\n","        window_sentences = sentences[start:end]\n","\n","        # Compute the features for the current window\n","        features = compute_features(window_sentences)\n","        features_list.append(features)\n","\n","        # Move the window one sentence forward\n","        start += 1\n","    # Store the features in the DataFrame\n","    window_features.append(features_list)"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of talks in training set: 976\n","Number of talks in testing set: 248\n"]}],"source":["features = ['FKRE_score', 'WPM', 'NAWL', 'NGSL', 'seconds', 'likes_per_view', 'laughter', 'applause', 'cheering', 'popularity_score']\n","\n","# Create a list of cluster labels and corresponding number of talks\n","cluster_labels = []\n","num_talks = []\n","for cluster, talks_nr in talks_per_cluster.items():\n","    cluster_labels.append(cluster)\n","    num_talks.append(talks_nr)\n","\n","# Create a list of indices for each cluster\n","cluster_indices = [np.where(data['popularity_category'] == label)[0] for label in cluster_labels]\n","\n","# Split each cluster into training and testing sets\n","train_indices = []\n","test_indices = []\n","for indices in cluster_indices:\n","    # Select the features and target variable for this cluster\n","    X = data.loc[indices, features]\n","    y = data.loc[indices, 'popularity_category']\n","\n","    # Normalize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Add the indices to the training and testing sets\n","    train_indices.extend(indices[X_train.astype(int)])\n","    test_indices.extend(indices[X_test.astype(int)])\n","\n","# Print the number of talks in the training and testing sets\n","print(f\"Number of talks in training set: {len(train_indices)}\")\n","print(f\"Number of talks in testing set: {len(test_indices)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["How to run CoreNLP Server\n","\n","cd .\\models\\stanford-corenlp-4.5.4\n","java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8080 -timeout 50000\n","\n","CoreNLP paper:\n","Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["more ciorna"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Thank', 'NNP'), ('you', 'PRP'), ('.', '.')]\n","[('It', 'PRP'), (\"'s\", 'VBZ'), ('really', 'RB'), ('great', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('here', 'RB'), ('.', '.')]\n","[('It', 'PRP'), (\"'s\", 'VBZ'), ('really', 'RB'), ('great', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('here', 'RB'), ('.', '.')]\n","[('I', 'PRP'), (\"'m\", 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('talk', 'VB'), ('to', 'TO'), ('you', 'PRP'), ('today', 'NN'), ('about', 'IN'), ('something', 'NN'), ('that', 'WDT'), (\"'s\", 'VBZ'), ('very', 'RB'), ('important', 'JJ'), ('to', 'TO'), ('me', 'PRP'), (',', ','), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('the', 'DT'), ('power', 'NN'), ('of', 'IN'), ('education', 'NN'), ('.', '.')]\n","[12, 13, 4, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["# Load the Stanford CoreNLP parser\n","# parser = CoreNLPParser(url='http://localhost:8080')\n","dep_parser = CoreNLPDependencyParser(url='http://localhost:8080')\n","\n","# Define the feature groups\n","syntactic_features = ['NP', 'VP', 'PP', 'SBAR', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NX', 'PRN', 'PRT', 'QP', 'RRC', 'UCP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP']\n","lexical_features = ['LV', 'LS', 'LS1', 'LS2', 'LS3', 'LD', 'LSO1', 'LSO2', 'LSO3', 'LSS1', 'LSS', 'LSS3']\n","ngram_features = ['spoken', 'magazine', 'fiction', 'news', 'academic']\n","\n","def parse_tree(tree):\n","    # Tree looks like \"(ROOT\\n  (S\\n    (VP (VBP Thank)\\n      (NP (PRP you)))\\n    (. .)))\"\n","    # Convert to NLTK Tree object\n","    return nltk.Tree.fromstring(tree)\n","\n","def compute_syntactic_features(parse_trees):\n","    syntactic_counts = Counter()\n","    for syntactic_feature in syntactic_features:\n","        syntactic_counts[syntactic_feature] = 0\n","    for tree_iterator in parse_trees:\n","        for tree in parse_tree(tree_iterator):\n","            for subtree in tree.subtrees():\n","                if subtree.label() in syntactic_features:\n","                    syntactic_counts[subtree.label()] += 1\n","    return syntactic_counts\n","\n","def compute_lexical_features(window):\n","    lexical_counts = Counter()\n","    for lexical_feature in lexical_features:\n","        lexical_counts[lexical_feature] = 0\n","    for sentence in window:\n","        tokens = nltk.word_tokenize(sentence)\n","        pos_tags = nltk.pos_tag(tokens)\n","        #print(pos_tags)\n","        for tag in pos_tags:\n","            if tag[1] in lexical_features:\n","                lexical_counts[tag[1]] += 1\n","    return lexical_counts\n","\n","def compute_ngram_features(window):\n","    ngram_counts = Counter()\n","    for ngram_feature in ngram_features:\n","        ngram_counts[ngram_feature] = 0\n","    for feature in ngram_features:\n","        for sentence in window:\n","            tokens = nltk.word_tokenize(sentence)\n","            ngrams = nltk.ngrams(tokens, n=2)\n","            #print(list(ngrams))\n","            freq_dist = nltk.FreqDist([ngram for ngram in ngrams if ngram[0].lower() in feature.split('_')])\n","            ngram_counts[feature] += freq_dist.B()\n","    return ngram_counts\n","\n","def extract_cocogen_features(text, ws=10):\n","    sentences = sent_tokenize(text)\n","    features = []\n","\n","    syntactic_counts = Counter()\n","    lexical_counts = Counter()\n","    ngram_counts = Counter()\n","\n","    for syntactic_feature in syntactic_features:\n","        syntactic_counts[syntactic_feature] = 0\n","\n","    for lexical_feature in lexical_features:\n","        lexical_counts[lexical_feature] = 0\n","\n","    for ngram_feature in ngram_features:\n","        ngram_counts[ngram_feature] = 0\n","\n","    for i in range(len(sentences) - ws + 1):\n","        window = sentences[i:i+ws]\n","        #parse_trees = parser.parse_sents(window)\n","        parse_trees = [sNLP.parse(sentence) for sentence in window]\n","\n","        sc = compute_syntactic_features(parse_trees)\n","        lc = compute_lexical_features(window)\n","        nc = compute_ngram_features(window)\n","\n","        for syntactic_feature in syntactic_features:\n","            syntactic_counts[syntactic_feature] += sc[syntactic_feature]\n","\n","        for lexical_feature in lexical_features:\n","            lexical_counts[lexical_feature] += lc[lexical_feature]\n","\n","        for ngram_feature in ngram_features:\n","            ngram_counts[ngram_feature] += nc[ngram_feature]\n","\n","    features.extend(syntactic_counts.values())\n","    features.extend(lexical_counts.values())\n","    features.extend(ngram_counts.values())\n","\n","    return features\n","\n","# Example usage\n","text = \"Thank you. It's really great to be here. I'm going to talk to you today about something that's very important to me, and that's the power of education.\"\n","features = extract_cocogen_features(text, 2)\n","print(features)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing rows:   0%|          | 3/1224 [02:33<17:23:08, 51.26s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(data\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing rows\u001b[39m\u001b[39m'\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m     transcript \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mraw_transcript\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     features \u001b[39m=\u001b[39m extract_cocogen_features(transcript, coco_window_size)\n\u001b[1;32m      9\u001b[0m     coco_features\u001b[39m.\u001b[39mappend(features)\n","Cell \u001b[0;32mIn[68], line 70\u001b[0m, in \u001b[0;36mextract_cocogen_features\u001b[0;34m(text, ws)\u001b[0m\n\u001b[1;32m     68\u001b[0m window \u001b[39m=\u001b[39m sentences[i:i\u001b[39m+\u001b[39mws]\n\u001b[1;32m     69\u001b[0m \u001b[39m#parse_trees = parser.parse_sents(window)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m parse_trees \u001b[39m=\u001b[39m [sNLP\u001b[39m.\u001b[39mparse(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m window]\n\u001b[1;32m     72\u001b[0m sc \u001b[39m=\u001b[39m compute_syntactic_features(parse_trees)\n\u001b[1;32m     73\u001b[0m lc \u001b[39m=\u001b[39m compute_lexical_features(window)\n","Cell \u001b[0;32mIn[68], line 70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m window \u001b[39m=\u001b[39m sentences[i:i\u001b[39m+\u001b[39mws]\n\u001b[1;32m     69\u001b[0m \u001b[39m#parse_trees = parser.parse_sents(window)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m parse_trees \u001b[39m=\u001b[39m [sNLP\u001b[39m.\u001b[39;49mparse(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m window]\n\u001b[1;32m     72\u001b[0m sc \u001b[39m=\u001b[39m compute_syntactic_features(parse_trees)\n\u001b[1;32m     73\u001b[0m lc \u001b[39m=\u001b[39m compute_lexical_features(window)\n","Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mStanfordNLP.parse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlp\u001b[39m.\u001b[39;49mparse(sentence)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/stanfordcorenlp/corenlp.py:205\u001b[0m, in \u001b[0;36mStanfordCoreNLP.parse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m--> 205\u001b[0m     r_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m'\u001b[39;49m\u001b[39mpos,parse\u001b[39;49m\u001b[39m'\u001b[39;49m, sentence)\n\u001b[1;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m [s[\u001b[39m'\u001b[39m\u001b[39mparse\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m r_dict[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m]][\u001b[39m0\u001b[39m]\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/stanfordcorenlp/corenlp.py:238\u001b[0m, in \u001b[0;36mStanfordCoreNLP._request\u001b[0;34m(self, annotators, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m\"\u001b[39m: kwargs[\u001b[39m'\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mproperties\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mstr\u001b[39m(properties), \u001b[39m'\u001b[39m\u001b[39mpipelineLanguage\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang}\n\u001b[1;32m    237\u001b[0m logging\u001b[39m.\u001b[39minfo(params)\n\u001b[0;32m--> 238\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, params\u001b[39m=\u001b[39;49mparams, data\u001b[39m=\u001b[39;49mdata, headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mConnection\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mclose\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[1;32m    239\u001b[0m r_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(r\u001b[39m.\u001b[39mtext)\n\u001b[1;32m    241\u001b[0m \u001b[39mreturn\u001b[39;00m r_dict\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:445\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    441\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n","File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:440\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    441\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define the sliding window size\n","coco_window_size = 2\n","coco_features = []\n","\n","# Iterate through the raw_transcript column\n","for index, row in tqdm.tqdm(data.iterrows(), total=data.shape[0], desc='Processing rows', position=0):\n","    transcript = row['raw_transcript']\n","    features = extract_cocogen_features(transcript, coco_window_size)\n","    coco_features.append(features)"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["scaler = MinMaxScaler (feature_range = (0,1))"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape:  (1224, 8, 1)\n","y shape:  (1224, 5, 1)\n","Train corpus size:  976\n","Test corpus size:  248\n"]}],"source":["def label_to_one_hot(label, num_classes=2):\n","    one_hot = [0] * num_classes\n","    one_hot[label] = 1\n","    return one_hot\n","\n","def one_hot_to_label(one_hot):\n","    one_hot = torch.tensor(one_hot)\n","    return torch.argmax(one_hot)\n","\n","def score(item):\n","    reactions = {'applause': 3, 'laughter': 2, 'cheering': 1}\n","    total_score = 0\n","    for key in reactions:\n","        total_score += item['total_responses'][key] * reactions[key]\n","    return total_score\n","\n","# features = ['FKRE_score', 'WPM', 'NAWL', 'NGSL', 'seconds', 'likes_per_view', 'laughter', 'applause', 'cheering', 'popularity_score']\n","\n","def extract_features_labels(data):\n","    X = []\n","    y = []\n","    for _, row in data.iterrows():\n","        features = []\n","        features.append(row[\"FKRE_score\"])\n","        # features.append(data[\"seconds\"])\n","        features.append(row[\"NAWL\"])\n","        features.append(row[\"NGSL\"])\n","        features.append(row[\"WPM\"])\n","        features.append(row[\"seconds\"])\n","        features.append(row[\"applause\"])\n","        features.append(row[\"laughter\"])\n","        features.append(row[\"cheering\"])\n","        X.append(features)\n","        # Like count to view count ratio\n","        y.append(label_to_one_hot(row[\"popularity_category\"], 5))\n","    X = np.array(X)\n","    y = np.array(y)\n","\n","    X = scaler.fit_transform(X)\n","    # y = scaler.fit_transform(y.reshape(-1,1))\n","\n","    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n","    y = np.reshape(y, (y.shape[0], y.shape[1], 1))\n","\n","    print(\"X shape: \", X.shape)\n","    print(\"y shape: \", y.shape)\n","\n","    return X, y\n","\n","X, y = extract_features_labels(data)\n","\n","# Create a list of cluster labels and corresponding number of talks\n","cluster_labels = []\n","num_talks = []\n","for cluster, talks_nr in talks_per_cluster.items():\n","    cluster_labels.append(cluster)\n","    num_talks.append(talks_nr)\n","\n","# Create a list of indices for each cluster\n","cluster_indices = [np.where(data['popularity_category'] == label)[0] for label in cluster_labels]\n","\n","# Split each cluster into training and testing sets\n","X_train = []\n","X_test = []\n","y_train = []\n","y_test = []\n","for indices in cluster_indices:\n","    xtrain, xtest, ytrain, ytest = train_test_split(X[indices], y[indices], test_size=0.2, random_state=42)\n","    X_train.extend(xtrain)\n","    X_test.extend(xtest)\n","    y_train.extend(ytrain)\n","    y_test.extend(ytest)\n","\n","X_train = np.array(X_train)\n","X_test = np.array(X_test)\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","\n","# # Perform KMeans clustering with 2 clusters\n","# kmeans = KMeans(n_clusters=5, random_state=0)\n","# kmeans.fit(np.reshape(y, (y.shape[0], y.shape[1])))\n","\n","# # Get the cluster centroids\n","# print(kmeans.cluster_centers_)\n","# # Get the cluster labels\n","# print(kmeans.labels_)\n","\n","# # Get the good and bad examples\n","# good_examples_X = X[kmeans.labels_ == 0]\n","# bad_examples_X = X[kmeans.labels_ == 1]\n","\n","# good_examples_y = y[kmeans.labels_ == 0]\n","# bad_examples_y = y[kmeans.labels_ == 1]\n","\n","# print(\"Good examples: \", good_examples_X.shape, good_examples_y.shape)\n","# print(\"Bad examples: \", bad_examples_X.shape, bad_examples_y.shape)\n","\n","# # Split the corpus into train and test sets\n","# X_train_good, X_test_good, y_train_good, y_test_good = train_test_split(good_examples_X, good_examples_y, test_size=0.2, shuffle=False)\n","# X_train_bad, X_test_bad, y_train_bad, y_test_bad = train_test_split(bad_examples_X, bad_examples_y, test_size=0.2, shuffle=False)\n","\n","# # Concatenate the good and bad examples\n","# X_train = np.concatenate((X_train_good, X_train_bad))\n","# X_test = np.concatenate((X_test_good, X_test_bad))\n","# y_train = np.concatenate((y_train_good, y_train_bad))\n","# y_test = np.concatenate((y_test_good, y_test_bad))\n","\n","print(\"Train corpus size: \", len(X_train))\n","print(\"Test corpus size: \", len(X_test))"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","31/31 [==============================] - 4s 47ms/step - loss: 0.1526\n","Epoch 2/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.1466\n","Epoch 3/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.1427\n","Epoch 4/30\n","31/31 [==============================] - 1s 20ms/step - loss: 0.1250\n","Epoch 5/30\n","31/31 [==============================] - 1s 19ms/step - loss: 0.1113\n","Epoch 6/30\n","31/31 [==============================] - 1s 19ms/step - loss: 0.1032\n","Epoch 7/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0954\n","Epoch 8/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0915\n","Epoch 9/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0932\n","Epoch 10/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0926\n","Epoch 11/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0910\n","Epoch 12/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0900\n","Epoch 13/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0890\n","Epoch 14/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0872\n","Epoch 15/30\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0871\n","Epoch 16/30\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0873\n","Epoch 17/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0895\n","Epoch 18/30\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0864\n","Epoch 19/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0886\n","Epoch 20/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0867\n","Epoch 21/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0873\n","Epoch 22/30\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0848\n","Epoch 23/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0833\n","Epoch 24/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0912\n","Epoch 25/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0834\n","Epoch 26/30\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0853\n","Epoch 27/30\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0843\n","Epoch 28/30\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0831\n","Epoch 29/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0830\n","Epoch 30/30\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0836\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x3785034c0>"]},"execution_count":210,"metadata":{},"output_type":"execute_result"}],"source":["regressor = Sequential ()\n","# TO DO Text vectorization\n","# TO DO Embedding layer\n","regressor.add(LSTM(units = 50, return_sequences= True, input_shape = (X_train.shape[1], 1)))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50, return_sequences= True))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50, return_sequences= True))\n","regressor.add(Dropout (0.2))\n","regressor.add(LSTM(units = 50))\n","regressor.add(Dropout (0.2))\n","regressor.add(Dense (units=5))\n","regressor.add(Softmax())\n","\n","regressor.compile(optimizer='adam', loss='mean_squared_error')\n","regressor.fit(X_train, y_train, epochs=30, batch_size=32)"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8/8 [==============================] - 0s 9ms/step\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  0 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  0 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  0 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  4 Actual:  [4]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  1 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  4 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  3 Actual:  [3]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  4 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  0 Actual:  [0]\n","Predicted:  3 Actual:  [1]\n","Predicted:  4 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  4 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  1 Actual:  [1]\n","Predicted:  3 Actual:  [1]\n","Predicted:  4 Actual:  [2]\n","Predicted:  3 Actual:  [2]\n","Predicted:  3 Actual:  [2]\n","Predicted:  4 Actual:  [2]\n","Predicted:  4 Actual:  [2]\n","MSE:  1.1935483870967742\n"]}],"source":["# Predict\n","y_pred = regressor.predict(X_test)\n","y_pred_argmax = np.argmax(y_pred, axis=1)\n","\n","# y_pred_normal = np.reshape(y_pred, (-1,1))\n","# y_test_normal = np.reshape(y_test, (-1,1))\n","\n","# y_pred_normal = scaler.inverse_transform(y_pred_normal)\n","# y_test_normal = scaler.inverse_transform(y_test_normal)\n","\n","# y_pred_normal = np.reshape(y_pred, (y_pred.shape[0],))\n","# y_test_normal = np.reshape(y_test, (y_test.shape[0],))\n","\n","y_test_argmax = np.argmax(y_test, axis=1)\n","\n","# Print predicted and actual values\n","for i in range(len(y_test_argmax)):\n","    print(\"Predicted: \", y_pred_argmax[i], \"Actual: \", y_test_argmax[i])\n","#print(\"MSE: \", mean_squared_error(y_pred_argmax, y_test_argmax))\n","print(\"Accuracy: \", accuracy_score(y_pred_argmax, y_test_argmax))\n","#print(\"Accuracy: \", accuracy_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2196017/2196017 [01:11<00:00, 30736.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 2195896 word vectors.\n"]}],"source":["embeddings_path = 'glove.840B.300d.txt'\n","with open(f'models/{embeddings_path}', 'r') as f:\n","    embeddings = {}\n","    lines = f.readlines()\n","    for line in tqdm.tqdm(lines):\n","        values = line.split()\n","        word = line.replace(\" \".join(values[-300:]), \"\").strip()\n","        vector = np.asarray(values[-300:], dtype='float32')\n","        embeddings[word] = vector\n","    word_to_ix = {key: i for i, key in enumerate(embeddings.keys())}\n","    embeddings_tensor = torch.tensor(list(embeddings.values()))\n","    print(\"Loaded %s word vectors.\" % len(embeddings))"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[],"source":["class BagOfSentencesModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, pretrained_embeddings):\n","        super(BagOfSentencesModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n","        self.lstm = nn.LSTM(input_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        #self.lstm1 = nn.LSTM(input_size, hidden_size)\n","        #self.lstm2 = nn.LSTM(hidden_size, hidden_size)\n","        #self.fc1 = nn.Linear(hidden_size, output_size * 10)\n","        #self.fc2 = nn.Linear(output_size * 10, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, sentences, argmax=False):\n","        sentence_embeddings = []\n","        for sentence in sentences:\n","            if not sentence: continue\n","            tokens = nltk.word_tokenize(sentence)\n","            tokens = [word for word in tokens if word in word_to_ix]\n","            word_indexes = torch.tensor([word_to_ix[word] for word in tokens], dtype=torch.long)\n","            embedded_sentence = self.embedding(word_indexes)\n","            # print(sentence)\n","            # print(tokens)\n","            # print(word_indexes)\n","            # print(embedded_sentence)\n","            _, (hidden_state, _) = self.lstm(embedded_sentence.view(len(word_indexes), 1, -1))\n","            #_, (hidden_state, _) = self.lstm2(hidden_state)\n","            sentence_embeddings.append(hidden_state[-1])\n","        h = torch.mean(torch.stack(sentence_embeddings), dim=0)\n","        r = self.fc(h)\n","        output = self.sigmoid(r)\n","        if argmax:\n","            return torch.argmax(output)\n","        return output\n","\n","    def predict(self, sentences, argmax=False):\n","        with torch.no_grad():\n","            # return argmax\n","            return self.forward(sentences, argmax)\n","        \n","    def predict_transcripts(self, transcripts, argmax=False):\n","        results = [self.predict(transcript, argmax) for transcript in transcripts]\n","        results = torch.stack(results).squeeze()\n","        return results\n","        \n","    def forward_transcripts(self, transcripts, argmax=False):\n","        results = [self.forward(transcript, argmax) for transcript in transcripts]\n","        results = torch.stack(results).squeeze()\n","        return results\n","        \n","    def predict_proba(self, sentences):\n","        return self.predict(sentences)\n","    \n","    def score(self, sentences, labels):\n","        predictions = self.predict(sentences)\n","        return accuracy_score(labels, predictions)\n","    \n","    def fit(self, transcripts, labels, epochs=1000, batch_size=32, loss_threshold=0.001, loss_threshold_epochs=10):\n","        criterion = nn.BCEWithLogitsLoss()\n","        optimizer = optim.Adam(self.parameters(), lr=0.001)\n","        prev_loss = 0\n","        sub_loss_threshold_epochs = loss_threshold_epochs\n","        for epoch in tqdm.tqdm(range(epochs)):\n","            for i in range(0, len(transcripts), batch_size):\n","                optimizer.zero_grad()\n","                batch_transcripts = transcripts[i:i+batch_size]\n","                batch_labels = labels[i:i+batch_size]\n","                predictions = self.forward_transcripts(batch_transcripts)\n","                batch_labels = torch.tensor(batch_labels, dtype=torch.float)\n","                # print(predictions, batch_labels)\n","                loss = criterion(predictions, batch_labels)\n","                loss.backward()\n","                optimizer.step()\n","                # Print statistics\n","                running_loss = loss.item()\n","                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n","                if abs(prev_loss - running_loss) < loss_threshold:\n","                    sub_loss_threshold_epochs -= 1\n","                    if sub_loss_threshold_epochs <= 0:\n","                        print(\"Loss threshold reached, stopping training.\")\n","                        return self\n","                else:\n","                    sub_loss_threshold_epochs = loss_threshold_epochs\n","                prev_loss = running_loss\n","        return self\n","model = BagOfSentencesModel(300, 128, 5, embeddings_tensor)"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10 10\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1000 [00:00<?, ?it/s]/var/folders/ks/8hgj6g0j5c5c88crmfrqcc_m0000gn/T/ipykernel_37260/4287664916.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch_labels = torch.tensor(batch_labels, dtype=torch.float)\n","  0%|          | 1/1000 [00:03<57:47,  3.47s/it]"]},{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 0.871\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [00:06<58:10,  3.50s/it]"]},{"name":"stdout","output_type":"stream","text":["[2,     1] loss: 0.863\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/1000 [00:10<57:30,  3.46s/it]"]},{"name":"stdout","output_type":"stream","text":["[3,     1] loss: 0.855\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 4/1000 [00:13<57:40,  3.47s/it]"]},{"name":"stdout","output_type":"stream","text":["[4,     1] loss: 0.848\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 5/1000 [00:17<57:51,  3.49s/it]"]},{"name":"stdout","output_type":"stream","text":["[5,     1] loss: 0.840\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 6/1000 [00:20<57:23,  3.46s/it]"]},{"name":"stdout","output_type":"stream","text":["[6,     1] loss: 0.831\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 7/1000 [00:24<58:08,  3.51s/it]"]},{"name":"stdout","output_type":"stream","text":["[7,     1] loss: 0.822\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 8/1000 [00:27<57:59,  3.51s/it]"]},{"name":"stdout","output_type":"stream","text":["[8,     1] loss: 0.812\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 9/1000 [00:31<57:51,  3.50s/it]"]},{"name":"stdout","output_type":"stream","text":["[9,     1] loss: 0.801\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 10/1000 [00:34<57:38,  3.49s/it]"]},{"name":"stdout","output_type":"stream","text":["[10,     1] loss: 0.790\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 11/1000 [00:38<57:21,  3.48s/it]"]},{"name":"stdout","output_type":"stream","text":["[11,     1] loss: 0.778\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 12/1000 [00:41<57:37,  3.50s/it]"]},{"name":"stdout","output_type":"stream","text":["[12,     1] loss: 0.765\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 13/1000 [00:45<57:28,  3.49s/it]"]},{"name":"stdout","output_type":"stream","text":["[13,     1] loss: 0.753\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 14/1000 [00:49<58:27,  3.56s/it]"]},{"name":"stdout","output_type":"stream","text":["[14,     1] loss: 0.743\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 15/1000 [00:52<59:20,  3.61s/it]"]},{"name":"stdout","output_type":"stream","text":["[15,     1] loss: 0.734\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 16/1000 [00:56<58:13,  3.55s/it]"]},{"name":"stdout","output_type":"stream","text":["[16,     1] loss: 0.726\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 17/1000 [00:59<57:55,  3.54s/it]"]},{"name":"stdout","output_type":"stream","text":["[17,     1] loss: 0.720\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 18/1000 [01:03<58:10,  3.55s/it]"]},{"name":"stdout","output_type":"stream","text":["[18,     1] loss: 0.715\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 19/1000 [01:06<57:15,  3.50s/it]"]},{"name":"stdout","output_type":"stream","text":["[19,     1] loss: 0.711\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 20/1000 [01:10<57:15,  3.51s/it]"]},{"name":"stdout","output_type":"stream","text":["[20,     1] loss: 0.708\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 21/1000 [01:13<57:57,  3.55s/it]"]},{"name":"stdout","output_type":"stream","text":["[21,     1] loss: 0.705\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 22/1000 [01:17<57:23,  3.52s/it]"]},{"name":"stdout","output_type":"stream","text":["[22,     1] loss: 0.703\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 23/1000 [01:21<58:14,  3.58s/it]"]},{"name":"stdout","output_type":"stream","text":["[23,     1] loss: 0.702\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 24/1000 [01:24<58:42,  3.61s/it]"]},{"name":"stdout","output_type":"stream","text":["[24,     1] loss: 0.700\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▎         | 25/1000 [01:28<58:18,  3.59s/it]"]},{"name":"stdout","output_type":"stream","text":["[25,     1] loss: 0.699\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 26/1000 [01:31<58:16,  3.59s/it]"]},{"name":"stdout","output_type":"stream","text":["[26,     1] loss: 0.698\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 27/1000 [01:35<59:10,  3.65s/it]"]},{"name":"stdout","output_type":"stream","text":["[27,     1] loss: 0.698\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 28/1000 [01:40<1:05:37,  4.05s/it]"]},{"name":"stdout","output_type":"stream","text":["[28,     1] loss: 0.697\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 29/1000 [01:44<1:02:42,  3.87s/it]"]},{"name":"stdout","output_type":"stream","text":["[29,     1] loss: 0.697\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 30/1000 [01:47<1:00:50,  3.76s/it]"]},{"name":"stdout","output_type":"stream","text":["[30,     1] loss: 0.696\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 31/1000 [01:51<59:55,  3.71s/it]  "]},{"name":"stdout","output_type":"stream","text":["[31,     1] loss: 0.696\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 32/1000 [01:54<59:00,  3.66s/it]"]},{"name":"stdout","output_type":"stream","text":["[32,     1] loss: 0.696\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 33/1000 [01:58<59:07,  3.67s/it]"]},{"name":"stdout","output_type":"stream","text":["[33,     1] loss: 0.695\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 34/1000 [02:01<57:48,  3.59s/it]"]},{"name":"stdout","output_type":"stream","text":["[34,     1] loss: 0.695\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 34/1000 [02:05<59:20,  3.69s/it]"]},{"name":"stdout","output_type":"stream","text":["[35,     1] loss: 0.695\n","Loss threshold reached, stopping training.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["BagOfSentencesModel(\n","  (embedding): Embedding(2195896, 300)\n","  (lstm): LSTM(300, 128)\n","  (fc): Linear(in_features=128, out_features=5, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["len_train_transcripts = 30\n","train_transcripts = []\n","for transcript in data[:len_train_transcripts]['transcript']:\n","    train_transcripts.append([sentence['sentence'] for sentence in transcript])\n","train_labels = data[:len_train_transcripts]['popularity_category']\n","# Filter only the labels equal to 0 or 1\n","filtered_train_labels = []\n","filtered_train_transcripts = []\n","for l, t in zip(train_labels, train_transcripts):\n","    if l in [0, 1]:\n","        filtered_train_labels.append(l)\n","        filtered_train_transcripts.append(t)\n","train_labels = filtered_train_labels\n","train_transcripts = filtered_train_transcripts\n","print(len(train_labels), len(train_transcripts))\n","# Convert labels to one-hot vectors\n","train_labels = [label_to_one_hot(label, 5) for label in train_labels]\n","train_labels = torch.tensor(train_labels, dtype=torch.float32)\n","#print(train_labels)\n","#print(list(word_to_ix.items())[:10])\n","model.fit(train_transcripts, train_labels)"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8 8\n","Train predictions:\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/ks/8hgj6g0j5c5c88crmfrqcc_m0000gn/T/ipykernel_37260/4287664916.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  one_hot = torch.tensor(one_hot)\n"]},{"name":"stdout","output_type":"stream","text":["Predicted 1, Actual 1, softmax layer: tensor([0.0443, 0.0464, 0.0062, 0.0066, 0.0042])\n","Predicted 1, Actual 1, softmax layer: tensor([0.0461, 0.0473, 0.0065, 0.0068, 0.0044])\n","Predicted 1, Actual 1, softmax layer: tensor([0.0442, 0.0466, 0.0062, 0.0067, 0.0042])\n","Predicted 1, Actual 1, softmax layer: tensor([0.0466, 0.0470, 0.0064, 0.0069, 0.0044])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0473, 0.0472, 0.0066, 0.0069, 0.0045])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0458, 0.0455, 0.0061, 0.0066, 0.0042])\n","Predicted 1, Actual 1, softmax layer: tensor([0.0444, 0.0461, 0.0061, 0.0066, 0.0042])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0483, 0.0449, 0.0063, 0.0067, 0.0043])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0488, 0.0473, 0.0068, 0.0071, 0.0046])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0479, 0.0468, 0.0067, 0.0069, 0.0045])\n","Test predictions:\n","Predicted 0, Actual 0, softmax layer: tensor([0.0465, 0.0435, 0.0060, 0.0063, 0.0041])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0468, 0.0450, 0.0063, 0.0065, 0.0043])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0478, 0.0452, 0.0064, 0.0067, 0.0043])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0463, 0.0445, 0.0061, 0.0064, 0.0041])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0461, 0.0446, 0.0061, 0.0064, 0.0042])\n","Predicted 1, Actual 1, softmax layer: tensor([0.0447, 0.0465, 0.0063, 0.0066, 0.0043])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0458, 0.0442, 0.0059, 0.0064, 0.0040])\n","Predicted 0, Actual 0, softmax layer: tensor([0.0468, 0.0453, 0.0062, 0.0066, 0.0042])\n"]}],"source":["len_test_transcripts = 30\n","test_transcripts = []\n","for transcript in data[len_train_transcripts:len_train_transcripts+len_test_transcripts]['transcript']:\n","    test_transcripts.append([sentence['sentence'] for sentence in transcript])\n","test_labels = data[len_train_transcripts:len_train_transcripts+len_test_transcripts]['popularity_category']\n","# Filter only the labels equal to 0 or 1\n","filtered_test_labels = []\n","filtered_test_transcripts = []\n","for l, t in zip(test_labels, test_transcripts):\n","    if l in [0, 1]:\n","        filtered_test_labels.append(l)\n","        filtered_test_transcripts.append(t)\n","test_labels = filtered_test_labels\n","test_transcripts = filtered_test_transcripts\n","print(len(test_labels), len(test_transcripts))\n","# Convert labels to one-hot vectors\n","test_labels = [label_to_one_hot(label, 5) for label in test_labels]\n","test_labels = torch.tensor(test_labels, dtype=torch.float32)\n","#print(test_labels)\n","#print(list(word_to_ix.items())[:10])\n","print(\"Train predictions:\")\n","predictions = model.predict_transcripts(train_transcripts)\n","for prediction, label in zip(predictions, train_labels):\n","    print(f\"Predicted {one_hot_to_label(prediction)}, Actual {one_hot_to_label(label)}, softmax layer: {prediction}\")\n","\n","print(\"Test predictions:\")\n","predictions = model.predict_transcripts(test_transcripts)\n","for prediction, label in zip(predictions, test_labels):\n","    print(f\"Predicted {one_hot_to_label(prediction)}, Actual {one_hot_to_label(label)}, softmax layer: {prediction}\")"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["import pickle\n","with open('models/model_1_0.pickle', 'wb') as f:\n","    f.write(pickle.dumps(model))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
